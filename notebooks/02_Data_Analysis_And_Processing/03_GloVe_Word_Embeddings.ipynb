{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, String, Integer, DATE, BOOLEAN\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../postgres.json') as pg_info:\n",
    "    pg_json = json.load(pg_info)\n",
    "    pg_username = pg_json['pg_username']\n",
    "    pg_password = pg_json['pg_password']\n",
    "    pg_ip = pg_json['pg_ip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class articles_raw(Base):\n",
    "    __tablename__ = 'arxiv_raw'\n",
    "    \n",
    "    id = Column(String, primary_key=True)\n",
    "    created = Column(DATE)\n",
    "    setspec = Column(String)\n",
    "    \n",
    "    title = Column(String)\n",
    "    title_converted = Column(BOOLEAN)\n",
    "    \n",
    "    abstract = Column(String)\n",
    "    abstract_converted = Column(BOOLEAN)\n",
    "\n",
    "class articles_detex(Base):\n",
    "    __tablename__ = 'arxiv_detex'\n",
    "    \n",
    "    id = Column(String, primary_key=True)\n",
    "    created = Column(DATE)\n",
    "    setspec = Column(String)\n",
    "    \n",
    "    title = Column(String)\n",
    "    title_converted = Column(BOOLEAN)\n",
    "    \n",
    "    abstract = Column(String)\n",
    "    abstract_converted = Column(BOOLEAN)\n",
    "\n",
    "class articles_pandoc(Base):\n",
    "    __tablename__ = 'arxiv_pandoc'\n",
    "    \n",
    "    id = Column(String, primary_key=True)\n",
    "    created = Column(DATE)\n",
    "    setspec = Column(String)\n",
    "    \n",
    "    title = Column(String)\n",
    "    title_converted = Column(BOOLEAN)\n",
    "    \n",
    "    abstract = Column(String)\n",
    "    abstract_converted = Column(BOOLEAN)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f'postgres://{pg_username}:{pg_password}@{pg_ip}:5432')\n",
    "Session = sessionmaker(bind=engine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over the query to process the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "for word in nlp.Defaults.stop_words:\n",
    "    lex = nlp.vocab[word]\n",
    "    lex.is_stop = True\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_abstract(abstract, nlp, white_space):\n",
    "    abstract = abstract.lower()\n",
    "    abstract = white_space.sub(' ', abstract)\n",
    "    \n",
    "    doc = nlp(abstract)\n",
    "    doc_filtered = []\n",
    "    for token in doc:\n",
    "        token_filter = (not token.is_punct) and (not token.is_stop)\n",
    "        \n",
    "        if token_filter:\n",
    "            doc_filtered.append(token.text)\n",
    "    return ' '.join(doc_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_over_table(table_query, corpus_file, text_processer, nlp):    \n",
    "    with open(corpus_file, 'w') as file:\n",
    "        for text_object in table_query.yield_per(1000):\n",
    "            doc = text_processer(text_object.abstract, nlp)\n",
    "            file.write(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_replace(match_obj):\n",
    "    str_to_pad = match_obj.group(0)\n",
    "    \n",
    "    begins = r'\\\\begin\\{.*?\\}'\n",
    "    ends = r'\\\\end\\{.*?\\}'\n",
    "    punct = '\\.\\,\\:'\n",
    "\n",
    "    \n",
    "    begin_match = re.match(begins, str_to_pad)\n",
    "    end_match = re.match(ends, str_to_pad)\n",
    "\n",
    "    if begin_match:\n",
    "        str_to_return  = ' ' + begin_match.group(0) + ' '\n",
    "        \n",
    "    elif end_match:\n",
    "        str_to_return  = ' ' + end_match.group(0) + ' '\n",
    "        \n",
    "    elif str_to_pad in punct:\n",
    "        str_to_return = ' '\n",
    "        \n",
    "    else:\n",
    "        str_to_return = f' {str_to_pad} '\n",
    "    return str_to_return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "table_query = session.query(articles_raw.abstract).limit(10)\n",
    "# %time loop_over_table(table_query, './test.txt', process_abstract, nlp)\n",
    "df = pd.read_sql(table_query.statement, table_query.session.bind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = df.loc[2,'abstract']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  The kappa - invariant and supersymmetric actions of D1 and D5 - branes in AdS_3 x\\nS^3 are investigated  as well as the action of a D5 - brane in an AdS_5 x S^5\\nbackground  The action of a D5 - brane lying totally in an AdS_3 x S^3 background\\nis found  Some progress was made towards finding the action for the D5 - brane\\nfree to move in the whole AdS_3 x S^3 x T^4 space  however the supersymmetric\\naction found here is not kappa - invariant and the reasons the method used did\\nnot find a kappa - invariant solution are discussed \\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_modes = r'\\$'\n",
    "\n",
    "begins = r'\\\\begin\\{.*?\\}'\n",
    "ends = r'\\\\end\\{.*?\\}'\n",
    "\n",
    "simple_math = '[+=\\-/]'\n",
    "functions = r'\\\\[a-zA-Z]+'\n",
    "punct = '[.,:]'\n",
    "\n",
    "pattern = f'{math_modes}|{begins}|{ends}|{simple_math}|{functions}|{punct}'\n",
    "\n",
    "\n",
    "\n",
    "finder = re.compile(pattern)\n",
    "finder.sub(match_replace, abstract)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arxiv_corpus(session, filepath, processer_function, compiled_regex, spacy_stopwords):\n",
    "    \n",
    "    table_query = session.query(articles_raw.abstract).yield_per(1000)\n",
    "    \n",
    "    \n",
    "    with open(file_path, 'w') as file:\n",
    "        for record in table_query:\n",
    "            abstract_string = record.abstract.lower().replace('\\n', ' ')\n",
    "            abstract_string = compiled_regex.sub(processer_function, abstract_string)\n",
    "            \n",
    "            doc = nlp(abstract_string)\n",
    "            \n",
    "            token_list = [token.lemma_.strip() for token in doc if not token.is_stop]\n",
    "            abstract_string = ' '.join(token_list)\n",
    "                    \n",
    "            \n",
    "            file.write(f' {abstract_string} ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg', disable=['parser'])\n",
    "\n",
    "#There's a bug in the current spaCy models that causes stop words to\n",
    "#not be set correctys\n",
    "for word in nlp.Defaults.stop_words:\n",
    "    lex = nlp.vocab[word]\n",
    "    lex.is_stop = True\n",
    "    \n",
    "    \n",
    "    \n",
    "len(nlp.Defaults.stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sesson = Session()\n",
    "# file_path = '../../vectors/arxiv_raw/corpus.txt'\n",
    "# processer_function = match_replace\n",
    "# compiled_regex = finder\n",
    "\n",
    "# %time arxiv_corpus(session, file_path, processer_function, compiled_regex, nlp.Defaults.stop_words)\n",
    "\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
