{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe Vectors\n",
    "\n",
    "In this notebook, we tokenize the article abstracts using the `re` module for regular expressions, use the `GloVe` library to create vectors for the tokens based on the corpus of abstracts, and then use these token vectors to create vectors for the associated with each article. This vector object is the primary mechanism of comparison for articles. In particular these vectors can literally just be thought of as geometrical vectors, and so should have an angle between them. This angle is precisely the quantity which the similarity score we will use, cosine similarity, implicitly measures. Two vectors have a higher similarity score when the angle between them is smaller. \n",
    "\n",
    "GloVe (short for Global Vectors), is a scheme for associating tokens to vectors by reading the entirety of the corpus of texts. GloVe is known to preserve semantic relationships between tokens, and so could do a good job of representing an entire abstract numerically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a SQLAlchemy `sessionmaker`\n",
    "The `sqlalchemy_load.py` file is setup to load a 1sessionmaker1 object which is already pointed at the `postgres` server. We'll use the `sessionmaker`, called Session, object to manage transactions with database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy_arxiv import Session, articles_raw, articles_vectors\n",
    "from sqlalchemy import func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the SQLAlchemy query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "query = session.query(articles_raw).limit(100)\n",
    "df = pd.read_sql(query.statement, query.session.bind)\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created</th>\n",
       "      <th>setspec</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1711.08738</td>\n",
       "      <td>2017-11-23</td>\n",
       "      <td>physics:physics</td>\n",
       "      <td>Complex Fluid-Fluid Interface may Non Triviall...</td>\n",
       "      <td>The present study theoretically predicts the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1711.08739</td>\n",
       "      <td>2017-11-23</td>\n",
       "      <td>physics:physics</td>\n",
       "      <td>Thermally modulated cross-stream migration of ...</td>\n",
       "      <td>In the present study, we investigate the cro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1711.08740</td>\n",
       "      <td>2017-11-23</td>\n",
       "      <td>cs</td>\n",
       "      <td>fpgaConvNet: A Toolflow for Mapping Diverse Co...</td>\n",
       "      <td>In recent years, Convolutional Neural Networ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1711.08741</td>\n",
       "      <td>2017-11-23</td>\n",
       "      <td>math</td>\n",
       "      <td>Remark on the strong solvability of the Navier...</td>\n",
       "      <td>The initial value problem of the incompressi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1711.08742</td>\n",
       "      <td>2017-11-23</td>\n",
       "      <td>cs</td>\n",
       "      <td>Estimating Missing Data in Temporal Data Strea...</td>\n",
       "      <td>Missing data is a ubiquitous problem. It is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id     created          setspec  \\\n",
       "0  1711.08738  2017-11-23  physics:physics   \n",
       "1  1711.08739  2017-11-23  physics:physics   \n",
       "2  1711.08740  2017-11-23               cs   \n",
       "3  1711.08741  2017-11-23             math   \n",
       "4  1711.08742  2017-11-23               cs   \n",
       "\n",
       "                                               title  \\\n",
       "0  Complex Fluid-Fluid Interface may Non Triviall...   \n",
       "1  Thermally modulated cross-stream migration of ...   \n",
       "2  fpgaConvNet: A Toolflow for Mapping Diverse Co...   \n",
       "3  Remark on the strong solvability of the Navier...   \n",
       "4  Estimating Missing Data in Temporal Data Strea...   \n",
       "\n",
       "                                            abstract  \n",
       "0    The present study theoretically predicts the...  \n",
       "1    In the present study, we investigate the cro...  \n",
       "2    In recent years, Convolutional Neural Networ...  \n",
       "3    The initial value problem of the incompressi...  \n",
       "4    Missing data is a ubiquitous problem. It is ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "For performing natural language processing, we first convert a text document, represented by a single string, and convert it into a sequence of strings, which are treated as the constituent objects making up the document. Since our textual data is highly specialized with specific types of formatting (mostly $\\LaTeX$ commands), I set up a tokenizer that tries to retain these relationships. \n",
    "\n",
    "For example, in mathematical writing, the string `\\mathbb{R}}`, which renders in $\\LaTeX$ as $\\mathbb{R}$, is typically used to refer to the real number line, while the string `R` by itself can refer to a number of very different objects (for example a [ring](https://en.wikipedia.org/wiki/Ring_(mathematics&#41;)). While the real number line is a ring, we absolutely do not want to conflate these two strings. So the idea here is to retain the `\\mathbb` command as a part of the whole token for the real number line. \n",
    "\n",
    "The functions in this section are built for the purpose of performing this tokenization which tries to respect the $LaTeX$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latex_repl(latex_string):\n",
    "    return \" \" + latex_string.group(0) + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latex():\n",
    "    #things to add spaces between\n",
    "    dollar_sign = r'\\${1,2}'\n",
    "    \n",
    "    parens = r\"\\\\\\(|\\\\\\)\"\n",
    "    \n",
    "    brackets_left = r\"\\\\[\"\n",
    "    brackets_right = r\"\\\\]\"\n",
    "    \n",
    "    #things like \\begins, \\mathbb, \\emph etc\n",
    "    commands = r'\\\\[a-zA-Z]+?\\{.*?\\}'\n",
    "    \n",
    "    \n",
    "    \n",
    "    constants_functions = r'\\\\[a-zA-Z]+'\n",
    "    simple_math = '[+=\\-/]'\n",
    "\n",
    "    pattern = [\n",
    "        dollar_sign,\n",
    "        brackets_right,\n",
    "        brackets_right,\n",
    "        parens,\n",
    "        commands,\n",
    "        constants_functions,\n",
    "        simple_math,\n",
    "    ]\n",
    "    \n",
    "    pattern = \"|\".join(pattern)\n",
    "\n",
    "    regex_compiled = re.compile(pattern, flags=re.DOTALL)\n",
    "    \n",
    "    return regex_compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_abstract(abstract, latex_regex, latex_repl, whitespace_regex):\n",
    "    abstract = latex_regex.sub(latex_repl, abstract)\n",
    "    abstract = whitespace_regex.sub(' ', abstract)\n",
    "    stop_words_removed = [ word for word in abstract.strip().split() if word.lower() not in ENGLISH_STOP_WORDS ]\n",
    "    return ' '.join(stop_words_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  The initial value problem of the incompressible Navier-Stokes equations with\\nnon-zero forces in $L^{n,\\\\infty}(\\\\mathbb{R}^n)$ is investigated. Even though\\nthe Stokes semigroup is not strongly continuous on\\n$L^{n,\\\\infty}(\\\\mathbb{R}^n)$, with the qualitative condition for the external\\nforces, it is clarified that the mild solution of the Naiver-Stokes equations\\nsatisfies the differential equations in the topology of\\n$L^{n,\\\\infty}(\\\\mathbb{R}^n)$. Inspired by the conditions for the forces, we\\ncharacterize the maximal complete subspace in $L^{n,\\\\infty}(\\\\mathbb{R}^n)$\\nwhere the Stokes semigroup is strongly continuous at $t=0$. By virtue of this\\nsubspace, we also show local well-posedness of the strong solvability of the\\nCauchy problem without any smallness condition on the initial data in the\\nsubspace. Finally, we discuss the uniqueness criterion for the mild solutions\\nin weak Lebesgue spaces by the argument by Brezis.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract = df.at[3,'abstract']\n",
    "abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'initial value problem incompressible Navier - Stokes equations non - zero forces $ L^{n \\\\infty }( \\\\mathbb{R} ^n) $ investigated Stokes semigroup strongly continuous $ L^{n \\\\infty }( \\\\mathbb{R} ^n) $ qualitative condition external forces clarified mild solution Naiver - Stokes equations satisfies differential equations topology $ L^{n \\\\infty }( \\\\mathbb{R} ^n) $ Inspired conditions forces characterize maximal complete subspace $ L^{n \\\\infty }( \\\\mathbb{R} ^n) $ Stokes semigroup strongly continuous $ t = 0 $ virtue subspace local - posedness strong solvability Cauchy problem smallness condition initial data subspace Finally discuss uniqueness criterion mild solutions weak Lebesgue spaces argument Brezis'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract = df.at[3,'abstract']\n",
    "\n",
    "latex_regex = latex()\n",
    "whitespace_regex = re.compile('[:;.,?\\s]+')\n",
    "\n",
    "process_abstract(abstract, latex_regex, latex_repl, whitespace_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a corpus text file.\n",
    "\n",
    "The `GloVe` library is set up to read a plain text file with documents separated by lines and tokens separated by spaces. The `table_processor` function reads the abstracts from the PostgreSQL database, tokenizes them as above, and writes them to a file so that we can use `GloVe`.\n",
    "\n",
    "Some of the processes below are a bit intensive. I ran this notebook in a `t2.large` instance on AWS. The `wall time` output below refers to actual time (measured by the computer), i.e. the time according to \"The clock on the wall\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_processor(session, table_class, corpus_file_path,\n",
    "                    batch_size, latex_regex, latex_repl, whitespace_regex):\n",
    "    \n",
    "    query = session.query(table_class.abstract).yield_per(batch_size)\n",
    "    \n",
    "    with open(corpus_file_path, 'w') as corpus:\n",
    "        for row in query:\n",
    "            abstract = row.abstract\n",
    "            abstract = process_abstract(abstract, latex_regex, latex_repl, whitespace_regex)\n",
    "            corpus.write(abstract + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options = {\n",
    "#     'session':Session(),\n",
    "#     'table_class':articles_raw,\n",
    "#     'corpus_file_path':'../../vectors/arxiv_corpus.txt',\n",
    "#     'batch_size':1000,\n",
    "#     'latex_regex':latex_regex,\n",
    "#     'latex_repl':latex_repl,\n",
    "#     'whitespace_regex':whitespace_regex,\n",
    "# }\n",
    "\n",
    "# %time table_processor(**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def train_GloVe():\n",
    "    cmd = [\"./glove_arxiv.sh\"]\n",
    "    with open(\"glove.log\", \"w\") as log:\n",
    "        subprocess.run(cmd, stderr=log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below took about 24 hours on a `t2.2xlarge` instance from AWS EC2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_GloVe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors for abstracts\n",
    "\n",
    "Reading the output of the `GloVe` vectors into a `pandas` DataFrame presented some odd challenges. Even set up correctly, specifying the proper separator etc., for some reason a small fraction of the rows were not read correctly, some index elements were doubled and some entire rows were read as indexes. To address this issue, I first tried to use the `np.genfromtext` method to read the file directly as an array. This also failed.\n",
    "\n",
    "Ultimately I just read the file in manually and explicitly constructed a DataFrame.\n",
    "\n",
    "I discovered this issue by looking at the lengths of different elements in the DataFrame index and saw a number of index elements (the `GloVe` tokens) that were more than one-hundred thousand characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../vectors/GloVe_scratch_files/vectors.txt', 'r') as file:\n",
    "    lines = file.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_split(line, cols, index):\n",
    "    label, *vector = line.split()\n",
    "    index.append(label)\n",
    "    \n",
    "    for col_num, new_value in enumerate(vector):\n",
    "        cols[col_num].append(new_value)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = {i:[] for i in range(300)}\n",
    "index = []\n",
    "\n",
    "for line in lines:\n",
    "    row_split(line, cols, index)\n",
    "\n",
    "df = pd.DataFrame(data=cols, index=index).astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-</th>\n",
       "      <td>0.221337</td>\n",
       "      <td>0.034171</td>\n",
       "      <td>-0.118100</td>\n",
       "      <td>-0.172244</td>\n",
       "      <td>-0.117517</td>\n",
       "      <td>0.128754</td>\n",
       "      <td>-0.116952</td>\n",
       "      <td>-0.028031</td>\n",
       "      <td>0.048693</td>\n",
       "      <td>0.010180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075310</td>\n",
       "      <td>0.199566</td>\n",
       "      <td>0.091558</td>\n",
       "      <td>-0.025151</td>\n",
       "      <td>-0.073502</td>\n",
       "      <td>-0.143817</td>\n",
       "      <td>-0.247823</td>\n",
       "      <td>0.089644</td>\n",
       "      <td>-0.154207</td>\n",
       "      <td>0.135924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$</th>\n",
       "      <td>0.570122</td>\n",
       "      <td>0.025272</td>\n",
       "      <td>-0.226462</td>\n",
       "      <td>0.163964</td>\n",
       "      <td>-0.141922</td>\n",
       "      <td>0.023358</td>\n",
       "      <td>-0.216404</td>\n",
       "      <td>-0.382666</td>\n",
       "      <td>-0.047065</td>\n",
       "      <td>0.433195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074485</td>\n",
       "      <td>0.425720</td>\n",
       "      <td>-0.032990</td>\n",
       "      <td>0.137178</td>\n",
       "      <td>-0.172863</td>\n",
       "      <td>0.279619</td>\n",
       "      <td>0.578546</td>\n",
       "      <td>-0.149470</td>\n",
       "      <td>0.093483</td>\n",
       "      <td>-0.060006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>0.230135</td>\n",
       "      <td>0.122644</td>\n",
       "      <td>0.185519</td>\n",
       "      <td>-0.034758</td>\n",
       "      <td>-0.236756</td>\n",
       "      <td>0.117980</td>\n",
       "      <td>-0.215023</td>\n",
       "      <td>-0.135311</td>\n",
       "      <td>0.253569</td>\n",
       "      <td>0.096707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036827</td>\n",
       "      <td>0.079115</td>\n",
       "      <td>-0.155606</td>\n",
       "      <td>-0.144959</td>\n",
       "      <td>0.156771</td>\n",
       "      <td>-0.164449</td>\n",
       "      <td>-0.417972</td>\n",
       "      <td>0.015737</td>\n",
       "      <td>-0.024429</td>\n",
       "      <td>0.098938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/</th>\n",
       "      <td>0.045956</td>\n",
       "      <td>0.271367</td>\n",
       "      <td>-0.392238</td>\n",
       "      <td>-0.259288</td>\n",
       "      <td>-0.272326</td>\n",
       "      <td>-0.161802</td>\n",
       "      <td>0.072062</td>\n",
       "      <td>-0.191248</td>\n",
       "      <td>-0.225739</td>\n",
       "      <td>-0.172222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005772</td>\n",
       "      <td>0.013629</td>\n",
       "      <td>0.012839</td>\n",
       "      <td>-0.014166</td>\n",
       "      <td>0.076333</td>\n",
       "      <td>0.026164</td>\n",
       "      <td>0.160343</td>\n",
       "      <td>0.112870</td>\n",
       "      <td>-0.216186</td>\n",
       "      <td>0.002026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>=</th>\n",
       "      <td>0.084244</td>\n",
       "      <td>0.014944</td>\n",
       "      <td>-0.228621</td>\n",
       "      <td>0.041487</td>\n",
       "      <td>-0.076570</td>\n",
       "      <td>-0.190362</td>\n",
       "      <td>-0.490148</td>\n",
       "      <td>-0.123931</td>\n",
       "      <td>0.126358</td>\n",
       "      <td>0.247899</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042797</td>\n",
       "      <td>0.206089</td>\n",
       "      <td>-0.154815</td>\n",
       "      <td>0.264402</td>\n",
       "      <td>-0.213689</td>\n",
       "      <td>0.084507</td>\n",
       "      <td>0.192470</td>\n",
       "      <td>-0.124478</td>\n",
       "      <td>-0.008774</td>\n",
       "      <td>-0.069953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "-      0.221337  0.034171 -0.118100 -0.172244 -0.117517  0.128754 -0.116952   \n",
       "$      0.570122  0.025272 -0.226462  0.163964 -0.141922  0.023358 -0.216404   \n",
       "model  0.230135  0.122644  0.185519 -0.034758 -0.236756  0.117980 -0.215023   \n",
       "/      0.045956  0.271367 -0.392238 -0.259288 -0.272326 -0.161802  0.072062   \n",
       "=      0.084244  0.014944 -0.228621  0.041487 -0.076570 -0.190362 -0.490148   \n",
       "\n",
       "            7         8         9      ...          290       291       292  \\\n",
       "-     -0.028031  0.048693  0.010180    ...     0.075310  0.199566  0.091558   \n",
       "$     -0.382666 -0.047065  0.433195    ...     0.074485  0.425720 -0.032990   \n",
       "model -0.135311  0.253569  0.096707    ...     0.036827  0.079115 -0.155606   \n",
       "/     -0.191248 -0.225739 -0.172222    ...     0.005772  0.013629  0.012839   \n",
       "=     -0.123931  0.126358  0.247899    ...    -0.042797  0.206089 -0.154815   \n",
       "\n",
       "            293       294       295       296       297       298       299  \n",
       "-     -0.025151 -0.073502 -0.143817 -0.247823  0.089644 -0.154207  0.135924  \n",
       "$      0.137178 -0.172863  0.279619  0.578546 -0.149470  0.093483 -0.060006  \n",
       "model -0.144959  0.156771 -0.164449 -0.417972  0.015737 -0.024429  0.098938  \n",
       "/     -0.014166  0.076333  0.026164  0.160343  0.112870 -0.216186  0.002026  \n",
       "=      0.264402 -0.213689  0.084507  0.192470 -0.124478 -0.008774 -0.069953  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `build_doc_vector` function takes an article abstract, represented as a string, and computes the vector associated with that artice. The `abstract_csv` function computes the vectors for all articles and writes them to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_doc_vector(document, word_df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the vector a given, already tokenized, document.\n",
    "    \"\"\"\n",
    "    document = document.split()\n",
    "    \n",
    "    vector = np.array([0.0 for _ in word_df.columns])\n",
    "    \n",
    "    for token in document:\n",
    "        if token in word_df.index:\n",
    "            vector += word_df.loc[token,:].values\n",
    "    return vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstract_csv(article_class,\n",
    "                 session,\n",
    "                 latex_regex,\n",
    "                 latex_repl,\n",
    "                 whitespace_regex,\n",
    "                 process_abstract,\n",
    "                 word_df,\n",
    "                 output_file\n",
    "                ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Write the article vectors to output_file.\n",
    "    \"\"\"\n",
    "    \n",
    "    query = session.query(article_class.id, article_class.abstract).yield_per(10000)\n",
    "    \n",
    "    with open(output_file, \"w\") as file:\n",
    "        for record in query:\n",
    "            abstract = process_abstract(record.abstract, latex_regex, latex_repl, whitespace_regex)\n",
    "            abstract_vector = build_doc_vector(abstract, word_df)\n",
    "            abstract_vector = list(abstract_vector)\n",
    "            abstract_vector = [str(vector_component) for vector_component in abstract_vector]\n",
    "\n",
    "            new_line = [record.id]\n",
    "            new_line.extend(abstract_vector)\n",
    "            new_line = ', '.join(new_line) + '\\n'\n",
    "            \n",
    "            file.write(new_line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below took about 6 hours to run on a `t2.large`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "output_file = \"../../vectors/arxiv_vectors.csv\"\n",
    "abstract_csv(articles_raw,\n",
    "             session,\n",
    "             latex_regex,\n",
    "             latex_repl,\n",
    "             whitespace_regex,\n",
    "             process_abstract,\n",
    "             word_df=df,\n",
    "             output_file=output_file,\n",
    ")\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass it back to the database\n",
    "\n",
    "We'll store the vectors associate with the articles in the our PostgreSQL database. The `csv` module is a convenient module for reading a `CSV` file directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def migrate_to_db(vector_file='../../vectors/arxiv_vectors.csv', session=None, articles_vectors=articles_vectors):\n",
    "    \n",
    "    \"\"\"\n",
    "    Sends teh vector_file to articles_vectors table in the \n",
    "    Postgres database. \n",
    "    \"\"\"\n",
    "    with open(vector_file, newline='', mode='r') as vector_csv:\n",
    "        csv_reader = csv.reader(vector_csv)\n",
    "        new_records = []\n",
    "        for line_num, line in enumerate(csv_reader):\n",
    "            \n",
    "            arxiv_id, *vector = line\n",
    "            vector = [float(comp) for comp in vector]\n",
    "            table_args = {f'comp_{i}':vector[i] for i in range(300) }\n",
    "            table_args['id'] = arxiv_id\n",
    "            new_table_entry = articles_vectors(**table_args)\n",
    "            new_records.append(new_table_entry)\n",
    "\n",
    "            #update every now and then.\n",
    "            if line_num % 1000 == 0:\n",
    "                session.add_all(new_records)\n",
    "                session.commit()\n",
    "            \n",
    "                new_records = []\n",
    "                \n",
    "        #final table update\n",
    "        session.add_all(new_records)\n",
    "        session.commit()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "%time migrate_to_db(session=session)\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
