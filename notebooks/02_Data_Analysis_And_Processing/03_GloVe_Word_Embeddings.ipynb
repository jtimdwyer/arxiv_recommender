{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, String, Integer, DATE, BOOLEAN\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../postgres.json') as pg_info:\n",
    "    pg_json = json.load(pg_info)\n",
    "    pg_username = pg_json['pg_username']\n",
    "    pg_password = pg_json['pg_password']\n",
    "    pg_ip = pg_json['pg_ip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class articles_raw(Base):\n",
    "    __tablename__ = 'arxiv_raw'\n",
    "    \n",
    "    id = Column(String, primary_key=True)\n",
    "    created = Column(DATE)\n",
    "    setspec = Column(String)\n",
    "    \n",
    "    title = Column(String)\n",
    "    title_converted = Column(BOOLEAN)\n",
    "    \n",
    "    abstract = Column(String)\n",
    "    abstract_converted = Column(BOOLEAN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f'postgres://{pg_username}:{pg_password}@{pg_ip}:5432')\n",
    "Session = sessionmaker(bind=engine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over the query to process the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "for word in nlp.Defaults.stop_words:\n",
    "    lex = nlp.vocab[word]\n",
    "    lex.is_stop = True\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_abstract(abstract, nlp, white_space):\n",
    "    abstract = abstract.lower()\n",
    "    abstract = white_space.sub(' ', abstract)\n",
    "    \n",
    "    doc = nlp(abstract)\n",
    "    doc_filtered = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop:\n",
    "            doc_filtered.append(token.text)\n",
    "            \n",
    "    return ' '.join(doc_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_over_table(table_query, corpus_file, text_processer, nlp): \n",
    "    white_space_processor = re.compile('\\s')\n",
    "    with open(corpus_file, 'w') as file:\n",
    "        for text_object in table_query.yield_per(1000):\n",
    "            doc = text_processer(text_object.abstract, nlp, white_space_processor)\n",
    "            file.write(doc + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_replace(match_obj):\n",
    "    str_to_pad = match_obj.group(0)\n",
    "    \n",
    "    begins = r'\\\\begin\\{.*?\\}'\n",
    "    ends = r'\\\\end\\{.*?\\}'\n",
    "    punct = '[\\'\";:/?.,`]'\n",
    "    \n",
    "\n",
    "    \n",
    "    begin_match = re.match(begins, str_to_pad)\n",
    "    end_match = re.match(ends, str_to_pad)\n",
    "\n",
    "    if begin_match:\n",
    "        str_to_return  = ' ' + begin_match.group(0) + ' '\n",
    "        \n",
    "    elif end_match:\n",
    "        str_to_return  = ' ' + end_match.group(0) + ' '\n",
    "        \n",
    "    elif str_to_pad in punct:\n",
    "        str_to_return = ' '\n",
    "        \n",
    "    else:\n",
    "        str_to_return = f' {str_to_pad} '\n",
    "    return str_to_return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arxiv_corpus(session, file_path, processer_function, compiled_regex, nlp):\n",
    "    \n",
    "    table_query = session.query(articles_raw.abstract).yield_per(1000)\n",
    "    \n",
    "    with open(file_path, 'w') as file:\n",
    "        for record in table_query:\n",
    "            abstract_string = record.abstract.lower().replace('\\n', ' ')\n",
    "            abstract_string = compiled_regex.sub(processer_function, abstract_string)\n",
    "            \n",
    "            doc = nlp(abstract_string)\n",
    "            \n",
    "            token_list = [token.lemma_.strip() for token in doc if not token.is_stop]\n",
    "            abstract_string = ' '.join(token_list)\n",
    "                    \n",
    "            \n",
    "            file.write(f'{abstract_string}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "math_modes = '\\$\\(\\)\\[\\]'\n",
    "\n",
    "begins = r'\\\\begin\\{.*?\\}'\n",
    "ends = r'\\\\end\\{.*?\\}'\n",
    "\n",
    "simple_math = '[+=\\-/]'\n",
    "functions = r'\\\\[a-zA-Z]+'\n",
    "punct = '[\\'\";:/?.,`]'\n",
    "\n",
    "pattern = f'{math_modes}|{begins}|{ends}|{simple_math}|{functions}|{punct}'\n",
    "\n",
    "finder = re.compile(pattern)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg', disable=['parser', 'tagger', 'ner'])\n",
    "\n",
    "#There's a bug in the current spaCy models that causes stop words to\n",
    "#not be set correctly\n",
    "for word in nlp.Defaults.stop_words:\n",
    "    lex = nlp.vocab[word]\n",
    "    lex.is_stop = True\n",
    "    \n",
    "    \n",
    "    \n",
    "len(nlp.Defaults.stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 50s, sys: 6.66 s, total: 17min 57s\n",
      "Wall time: 18min 15s\n"
     ]
    }
   ],
   "source": [
    "session = Session()\n",
    "%time arxiv_corpus(session, './test.txt', match_replace, finder, nlp)\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
