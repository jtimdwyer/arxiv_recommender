{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity of documents\n",
    "\n",
    "In the [data analysis and processing directory](./02_Data_Analysis_And_Processing) we created a vector to associate with each article which, in some sense, attempts to capture the content of the article (or at least the content of the abstract). So when we talk about whether or not two articles are similar, we're really talking about the similarity of the vectors. A standard way of evaluating the similarity between two vectors is looking at the angle between them. This should be motivated by the fact that parallel vectors seem like they should be similar while orthogonal vectors are dissimilar. Numerically we represent this quantity by considering the normalized dot product of the two vectors (this number, normalized correctly, is the cosine of the angle between the vectors). \n",
    "\n",
    "The normalized dot product of two vectors can take values between $-1$ and $1$, with $1$ corresponding to parallel vectors (high similarity of articles) and $-1$ corresponding to vectors pointing in opposite directions (very dissimilar articles). All of the scores we use refer to the cosine similarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep the data for computation\n",
    "\n",
    "We use `numpy` to perform fairly efficient numerical matrix operations and some sorting. These mathematical operations translate, from the lens of recommending articles to a user, to computing similarity scores and then sorting the articles by their score. \n",
    "\n",
    "The `json` and `pickle` libraries are used to persist the objects used in these computations, since we will need to reuse them to update the database. Also while preprocessing is running, we can have the objects stored in memory on the Flask app serving an API which will allow us to compute recommendations online. \n",
    "\n",
    "The `csv` module is used to simplify reading the `CSV` file which stores the article vectors.\n",
    "\n",
    "The `pandas` module is used here to examing some sample recommendations.\n",
    "\n",
    "For the sake of storage and memory, we'll only keep the top 50 rated recommendations for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vectors_ids(vectors_file='../../vectors/arxiv_vectors.csv',\n",
    "                      id_json='../../vectors/id.json',\n",
    "                      vectors_pkl='../../vectors/vectors.pkl',\n",
    "                     ):\n",
    "    \"\"\"\n",
    "    Takes in the total csv of vectors for \n",
    "    the articles, indexed by arxiv_id and \n",
    "    transforms them into a list of ids\n",
    "    and array of vectors. Then the array is \n",
    "    pickled and the ids list is stored as \n",
    "    a json.\n",
    "    \"\"\"\n",
    "    \n",
    "    ids = []\n",
    "    vectors = []\n",
    "    \n",
    "    with open(vectors_file, 'r', newline='') as vectors_csv:\n",
    "        vectors_reader = csv.reader(vectors_csv)\n",
    "        for id, *vector in vectors_reader:\n",
    "            ids.append(id)\n",
    "            vector = np.array([float(component) for component in vector])\n",
    "            vectors.append(vector)\n",
    "    \n",
    "    with open(id_json, 'w') as json_file:\n",
    "        json.dump(ids, json_file)\n",
    "    \n",
    "    vectors = np.array(vectors)\n",
    "    vectors = normalize_rows(vectors)\n",
    "    \n",
    "    with open(vectors_pkl, 'bw') as pkl_file:\n",
    "        pickle.dump(vectors, pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_vectors_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a bug with loading large pickled files on MacOS, so id you try to run this locally on a Mac, it probably won't work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../vectors/vectors.pkl', 'rb') as vec_pkl:\n",
    "    vectors = pickle.load(vec_pkl)\n",
    "    \n",
    "with open('../../vectors/id.json', 'r') as ids_json:\n",
    "    all_ids = json.load(ids_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Scores\n",
    "\n",
    "The functions in this subsection serve the following purpose\n",
    "\n",
    "1. `score` - compute a batch similarity scores \n",
    "1. `sort_scores` - sort the similarity scores, keeping in mind which articles the scores are associated with\n",
    "1. `compute_recs_paper_id` - A convenience function for computing the recommendations of an article by specifying the id of the article. This will be used in the Flask app for online computation of recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(vectors, all_ids, id_low, id_high):\n",
    "    \"\"\"\n",
    "    Outputs an array, scores. The columns of scores\n",
    "    are correspond to the slice of article_ids\n",
    "\n",
    "    all_id[id_low:id_high]\n",
    "\n",
    "    The rows are indexed by all_ids.\n",
    "\n",
    "    The value in column C and row R is the\n",
    "    similarity between the articles all_ids[R]\n",
    "    and all_ids[id_low+C].\n",
    "    \"\"\"\n",
    "    mask = [id_low <= index < id_high for index, _ in enumerate(all_ids)]\n",
    "    rows = vectors[mask]\n",
    "    scores = vectors @ rows.T\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_scores(scores, all_ids, cur_ids):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of lists. The keys are the ids\n",
    "    of articles we're currently evaluating.\n",
    "\n",
    "    The lists contain tuples of scores and ids. The\n",
    "    score is the similarity score between the current article\n",
    "    and the other component of the tuple.\n",
    "    \"\"\"\n",
    "    recs_index = scores.argsort(0)[-51:,:][::-1]\n",
    "    recs = {\n",
    "      id:[(all_ids[index], scores[index][0]) for index in recs_index[:, col_num]]\n",
    "        for col_num, id in enumerate(cur_ids)\n",
    "    }\n",
    "    return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_recs_batch(all_ids, vectors, low, high):\n",
    "    \"\"\"\n",
    "    Computes the recommendations for the articles in\n",
    "    the list all_ids[low:high].\n",
    "    Returns a dictionary that can be fed into\n",
    "    send_to_server for upload to the postgres database.\n",
    "    \"\"\"\n",
    "    scored = score(vectors, all_ids, low, high)\n",
    "    recs = sort_scores(scored, all_ids, all_ids[low:high])\n",
    "    return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 800 ms, sys: 0 ns, total: 800 ms\n",
      "Wall time: 533 ms\n"
     ]
    }
   ],
   "source": [
    "%time recs = compute_recs_paper_id('1801.08262', all_ids, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(recs['1801.08262'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1801.08262', 1.0000000000000002),\n",
       " ('1212.2697', 0.96994439808605737),\n",
       " ('1410.0230', 0.9675059791477697),\n",
       " ('1611.03840', 0.96315506293288022),\n",
       " ('1705.00164', 0.96170088971694145)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs['1801.08262'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with SQL\n",
    "\n",
    "We'll define some functions that allow us to store the recommendations in a database so we don't have to compute them again when they're requested by the flask app. In particular the flask app should be able to perform two specific operations with the database.\n",
    "\n",
    "1. We should be able to retrieve check if a record exists in the table, and if it does render something for the user.\n",
    "2. If there is not matching record in the table, we can update the table and then service the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy_arxiv import Session, articles_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_server(recs, table_class_recs, session):\n",
    "    \"\"\"\n",
    "    Sends computed recommendations to the the database\n",
    "    \"\"\"\n",
    "    new_recs = [{'id':key,'recs':value} for key, value in recs.items()]\n",
    "    new_recs = [table_class_recs(**args) for args in new_recs]\n",
    "    for new in new_recs:\n",
    "        session.merge(new)\n",
    "    session.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the new recommendations to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "send_to_server(recs, articles_similar, session)\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the records back from the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_recs(id_request, table_class_recs, session):\n",
    "    \"\"\"\n",
    "    Retrieves computed recommendations from the database\n",
    "    \"\"\"\n",
    "    query = (session\n",
    "             .query(table_class_recs)\n",
    "             .filter(table_class_recs.id==id_request)\n",
    "            )\n",
    "    records = query.all()\n",
    "    if records:\n",
    "        return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "recs_ = request_recs('1801.08262', articles_similar, session)\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1801.08262', 1.0000000000000002],\n",
       " ['1212.2697', 0.9699443980860574],\n",
       " ['1410.0230', 0.9675059791477697],\n",
       " ['1611.03840', 0.9631550629328802],\n",
       " ['1705.00164', 0.9617008897169415]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs_[0].recs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random articles results\n",
    "\n",
    "We'll take a concrete look at a random sample of recommendations of articles, and see how they stack up. First let's get a random articlea and look at their top 5 similar articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(40)\n",
    "random_ids = np.random.choice(all_ids, size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is effectively a simulation of how we would respond to a request for a recommendation in practice. A user would provide an `arxiv_id` and we would respond with other `arxiv_ids`, hopefully as a link and with things like the title and abstract of the recommended articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'quant-ph/0307015'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_recs = []\n",
    "for id in random_ids:\n",
    "    article_index = all_ids.index(id)\n",
    "    tmp = score(vectors, all_ids, article_index,  article_index+1)\n",
    "    tmp = sort_scores(tmp, all_ids, [id])\n",
    "    tmp[id] = tmp[id][:6]\n",
    "    random_recs.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quant-ph/0307015    1.000000\n",
       "quant-ph/0509075    0.908993\n",
       "quant-ph/0406008    0.894806\n",
       "quant-ph/0611137    0.891376\n",
       "quant-ph/0207112    0.888474\n",
       "quant-ph/9712020    0.887291\n",
       "dtype: float64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_dict = random_recs[0]\n",
    "pd.Series(data=[x[1] for x in list(rec_dict.values())[0]], index=[x[0] for x in list(rec_dict.values())[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you follow any of the ids above to their pages on arxiv, ie go to https://arxiv.org/abs/quant-ph/0307015, you will find the following titles:\n",
    "1. [Bounds on the Probability of Success of Postselected Non-linear Sign Shifts Implemented with Linear Optics](https://arxiv.org/abs/quant-ph/0307015)\n",
    "1. [Feed-forward and its role in conditional linear optical quantum dynamics](https://arxiv.org/abs/quant-ph/0509075)\n",
    "1. [An efficient quantum filter for multiphoton states](https://arxiv.org/abs/quant-ph/0406008)\n",
    "1. [Minimum-energy pulses for quantum logic cannot be shared](https://arxiv.org/abs/quant-ph/0611137)\n",
    "1. [Linear optics implementation of general two-photon projective measurement](https://arxiv.org/abs/quant-ph/0207112)\n",
    "1. [Optimal Signal-to-Quantum Noise Ratio for Nonclassical Number States](https://arxiv.org/abs/quant-ph/9712020)\n",
    "\n",
    "Keep in mind, the first article in this list is the article we're providing recommendations for. A quick read of the abstracts of these articles tells me that they're all related to optics and and photon gates. I would say these recommendations are good, and that these paper are certainly similar, but I can't really judge how perfectly aligned they are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing a non-random article\n",
    "There are some articles I can personally judge, as a subject matter expert, as being good recommendations. For example, there is a paper I wrote recently, with my former advisor Sergi Elizalde, titled [Wilf equivalence relations for consecutive patterns](https://arxiv.org/abs/1801.08262). Let's look at the recommendations this system provides for that paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_paper = '1801.08262'\n",
    "article_index = all_ids.index(my_paper)\n",
    "tmp = score(vectors, all_ids, article_index,  article_index+1)\n",
    "tmp = sort_scores(tmp, all_ids, [my_paper])\n",
    "tmp[my_paper] = tmp[my_paper][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1801.08262    1.000000\n",
       "1212.2697     0.969944\n",
       "1410.0230     0.967506\n",
       "1611.03840    0.963155\n",
       "1705.00164    0.961701\n",
       "0805.1872     0.961022\n",
       "dtype: float64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_dict = tmp\n",
    "pd.Series(data=[x[1] for x in list(rec_dict.values())[0]], index=[x[0] for x in list(rec_dict.values())[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same thing we did above, the following list of articles begins with the article we're finding recommendations for, and the remainder of the list are the recommended articles. I can, without qualificiation, say that these are good recommendations. All of these papers absolutely are a part of a topic in combinatorial mathematics called __permutation patterns__. This is a very specialized sub-field of mathematics, and the recommender is definitely staying within this sub-field. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Wilf equivalence relations for consecutive patterns](https://arxiv.org/abs/1801.08262)\n",
    "1. [On Pattern Avoiding Alternating Permutations](https://arxiv.org/abs/1212.2697)\n",
    "1. [Egge triples and unbalanced Wilf-equivalence](https://arxiv.org/abs/1410.0230)\n",
    "1. [The Length of the Longest Common Subsequence of Two Independent Mallows Permutations](https://arxiv.org/abs/1611.03840)\n",
    "1. [Quadrant marked mesh patterns in 123-avoiding permutations](https://arxiv.org/abs/1705.00164)\n",
    "1. [Avoidance of Partially Ordered Generalized Patterns of the form  k-σ-k](https://arxiv.org/abs/0805.1872)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A non-example\n",
    "\n",
    "There's no way the recommender will always give results as good as the ones above. Finding an example of this can be a little complicated. Below I compute recommendations for an article that I read in graduate school and genuinely had trouble making sense of and tried to find other sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_paper = 'math/9806036'\n",
    "article_index = all_ids.index(my_paper)\n",
    "tmp = score(vectors, all_ids, article_index,  article_index+1)\n",
    "tmp = sort_scores(tmp, all_ids, [my_paper])\n",
    "tmp[my_paper] = tmp[my_paper][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "math/9806036    1.000000\n",
       "1106.5646       0.847961\n",
       "cs/0512073      0.820467\n",
       "1106.5531       0.818694\n",
       "1401.1089       0.814763\n",
       "0801.3194       0.803516\n",
       "dtype: float64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_dict = tmp\n",
    "pd.Series(data=[x[1] for x in list(rec_dict.values())[0]], index=[x[0] for x in list(rec_dict.values())[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [The Goulden-Jackson Cluster Method: Extensions, Applications and Implementations](https://arxiv.org/abs/math/9806036)\n",
    "1. [The Number of Same-Sex Marriages in a Perfectly Bisexual Population is Asymptotically Normal](https://arxiv.org/abs/1106.5646)\n",
    "1. [Schwerdtfeger-Fillmore-Springer-Cnops Construction Implemented in GiNaC](https://arxiv.org/abs/cs/0512073)\n",
    "1. [Balls in Boxes: Variations on a Theme of Warren Ewens and Herbert Wilf](https://arxiv.org/abs/1106.5531)\n",
    "1. [Automatic Enumeration of Generalized Menage Numbers](https://arxiv.org/abs/1401.1089)\n",
    "1. [The Fedosov *-product in Mathematica](https://arxiv.org/abs/0801.3194)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores here are much lower than the previous examples, which we would hope indicates that the matches are not as good. This is indeed the case! Looking at the abstracts of these papers nothing about the content that indicates a strong relationship between these articles and the the article we're finding recommendations for. There is however on striking similarity in all of these abstracts. The all contain the phrase `this http url` in reference to a link. I have no idea if this is some old school way of describing links, but this idiom really seems to have boosted the scores between these articles. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
