{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity of documents\n",
    "\n",
    "We'll store the documents locally and put recommendations in the postgres server for easier access with the app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep the data for computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_rows(matr):\n",
    "    \"\"\"\n",
    "    Normalize the rows of a 2-D array,\n",
    "    avoiding divisions by 0\n",
    "    \"\"\"\n",
    "    @np.vectorize\n",
    "    def fix_normalize(norm):\n",
    "        if norm > 0:\n",
    "            return norm\n",
    "        else: \n",
    "            return 1\n",
    "    norms = np.linalg.norm(matr, ord=2, axis=1).reshape(-1,1)\n",
    "    norms = fix_normalize(norms)\n",
    "    matr = matr/norms\n",
    "    return matr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vectors_ids(vectors_file='../../vectors/arxiv_vectors.csv',\n",
    "             id_json='../../vectors/id.json',\n",
    "             vectors_pkl='../../vectors/vectors.pkl',\n",
    "            ):\n",
    "    \"\"\"\n",
    "    Takes in the total csv of vectors for \n",
    "    the articles, indexed by arxiv_id and \n",
    "    transforms them into a list of ids\n",
    "    and array of vectors. Then the array is \n",
    "    pickled and the ids list is stored as \n",
    "    a json.\n",
    "    \"\"\"\n",
    "    \n",
    "    ids = []\n",
    "    vectors = []\n",
    "    \n",
    "    with open(vectors_file, 'r', newline='') as vectors_csv:\n",
    "        vectors_reader = csv.reader(vectors_csv)\n",
    "        for id, *vector in vectors_reader:\n",
    "            ids.append(id)\n",
    "            vector = np.array([float(component) for component in vector])\n",
    "            vectors.append(vector)\n",
    "    \n",
    "    with open(id_json, 'w') as json_file:\n",
    "        json.dump(ids, json_file)\n",
    "    \n",
    "    vectors = np.array(vectors)\n",
    "    vectors = normalize_rows(vectors)\n",
    "    \n",
    "    with open(vectors_pkl, 'bw') as pkl_file:\n",
    "        pickle.dump(vectors, pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_vectors_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../vectors/vectors.pkl', 'rb') as vec_pkl:\n",
    "    vectors = pickle.load(vec_pkl)\n",
    "    \n",
    "with open('../../vectors/id.json', 'r') as ids_json:\n",
    "    ids = json.load(ids_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(vectors, all_ids, id_low, id_high):\n",
    "    \"\"\"\n",
    "    Outputs an array, scores. The columns of scores \n",
    "    are correspond to the slice of article_ids \n",
    "    \n",
    "    all_id[id_low:id_high]\n",
    "    \n",
    "    The rows are indexed by all_ids.\n",
    "    \n",
    "    The value in column C and row R is the \n",
    "    similarity between the articles all_ids[R]\n",
    "    and all_ids[id_low+C].\n",
    "    \"\"\"\n",
    "    mask = [id_low <= index < id_high for index, _ in enumerate(all_ids)]\n",
    "    rows = vectors[mask]\n",
    "    scores = vectors @ rows.T\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_scores(scores, all_ids, cur_ids):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of lists. The keys are the ids \n",
    "    of articles we're currently evaluating.\n",
    "    \n",
    "    The lists contain tuples of scores and ids. The \n",
    "    score is the similarity score between the current article\n",
    "    and the other component of the tuple.\n",
    "    \"\"\"\n",
    "    recs = {}\n",
    "    for col_num, col in enumerate(scores.T):\n",
    "        article_id = cur_ids[col_num]\n",
    "        recs[article_id] = list(zip(col, ids))\n",
    "        recs[article_id].sort(key=lambda x: x[0], reverse=True)\n",
    "        recs[article_id] = recs[article_id][:51]\n",
    "    return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_recs_paper_id(arxiv_id, ids, vectors):\n",
    "    \"\"\"\n",
    "    Takes the list of id_number in question, \n",
    "    the list ids, and array of vectors\n",
    "    and returns a dictionary whose key is the arxiv_id\n",
    "    and the only value is the \n",
    "    \"\"\"\n",
    "    \n",
    "    id_num = ids.index(arxiv_id)\n",
    "    scored = score(vectors, ids, id_num, id_num+1)\n",
    "    recs = sort_scores(scored, ids, ids[id_num:id_num+1])\n",
    "    return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.81 s, sys: 692 ms, total: 3.5 s\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%time recs = get_recs_paper_id('1801.08262', ids, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(recs['1801.08262'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.01 s, sys: 568 ms, total: 3.58 s\n",
      "Wall time: 2.34 s\n"
     ]
    }
   ],
   "source": [
    "%time recs = get_recs_paper_id('1805.06077', ids, vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with SQL\n",
    "\n",
    "We'll define some functions that allow us to store the recommendations in a database so we don't have to compute them again when they're requested by the flask app. In particular the flask app should be able to perform two specific operations with the database.\n",
    "\n",
    "1. We should be able to retrieve check if a record exists in the table, and if it does render something for the user.\n",
    "2. If there is not matching record in the table, we can update the table and then service the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy_arxiv import Session, articles_similar, Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "args = {\n",
    "    'id':'1801.08262',\n",
    "    'recs':recs['1801.08262'],\n",
    "}\n",
    "\n",
    "session.add(articles_similar(**args))\n",
    "session.commit()\n",
    "session.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "r = session.query(articles_similar).all()\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_server(arxiv_id, recs, table_class, session):\n",
    "    recs = recs[arxiv_id]\n",
    "    recs = recs[(str(score))]\n",
    "    new_recs = {\n",
    "        'id':id_request,\n",
    "        'recs':recs_p\n",
    "    }\n",
    "    new_recs = table_class(**new_recs)\n",
    "    session.add(new_recs)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_record = all_ids_list[5]\n",
    "\n",
    "session = Session()\n",
    "recs = create_sims_from_id(id_record, all_ids_list=all_ids_list)\n",
    "send_to_server(id_record, recs, articles_similar, session)\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_recs(id_request, table_class, session):\n",
    "    query = session.query(table_class).filter(table_class.id==id_request)\n",
    "    records = query.all()[0].recs.split()\n",
    "    records = list(zip(records[0::2], records[1::2]))\n",
    "    records = [list(x) for x in records]\n",
    "    return records\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
