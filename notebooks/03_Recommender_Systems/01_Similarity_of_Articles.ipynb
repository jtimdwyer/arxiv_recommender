{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity of documents\n",
    "\n",
    "In the [data analysis and processing directory](./02_Data_Analysis_And_Processing) we created a vector to associate with each article which, in some sense, attempts to capture the content of the article (or at least the content of the abstract). So when we talk about whether or not two articles are similar, we're really talking about the similarity of the vectors. A standard way of evaluating the similarity between two vectors is looking at the angle between them. This should be motivated by the fact that parallel vectors seem like they should be similar while orthogonal vectors are dissimilar. Numerically we represent this quantity by considering the normalized dot product of the two vectors (this number, normalized correctly, is the cosine of the angle between the vectors). \n",
    "\n",
    "The normalized dot product of two vectors can take values between $-1$ and $1$, with $1$ corresponding to parallel vectors (high similarity of articles) and $-1$ corresponding to vectors pointing in opposite directions (very dissimilar articles). All of the scores we use refer to the cosine similarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep the data for computation\n",
    "\n",
    "We use `numpy` to perform fairly efficient numerical matrix operations and some sorting. These mathematical operations translate, from the lens of recommending articles to a user, to computing similarity scores and then sorting the articles by their score. \n",
    "\n",
    "The `json` and `pickle` libraries are used to persist the objects used in these computations, since we will need to reuse them to update the database. Also while preprocessing is running, we can have the objects stored in memory on the Flask app serving an API which will allow us to compute recommendations online. \n",
    "\n",
    "The `csv` module is used to simplify reading the `CSV` file which stores the article vectors.\n",
    "\n",
    "\n",
    "For the sake of storage and memory, we'll only keep the top 50 rated recommendations for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vectors_ids(vectors_file='../../vectors/arxiv_vectors.csv',\n",
    "                      id_json='../../vectors/id.json',\n",
    "                      vectors_pkl='../../vectors/vectors.pkl',\n",
    "                     ):\n",
    "    \"\"\"\n",
    "    Takes in the total csv of vectors for \n",
    "    the articles, indexed by arxiv_id and \n",
    "    transforms them into a list of ids\n",
    "    and array of vectors. Then the array is \n",
    "    pickled and the ids list is stored as \n",
    "    a json.\n",
    "    \"\"\"\n",
    "    \n",
    "    ids = []\n",
    "    vectors = []\n",
    "    \n",
    "    with open(vectors_file, 'r', newline='') as vectors_csv:\n",
    "        vectors_reader = csv.reader(vectors_csv)\n",
    "        for id, *vector in vectors_reader:\n",
    "            ids.append(id)\n",
    "            vector = np.array([float(component) for component in vector])\n",
    "            vectors.append(vector)\n",
    "    \n",
    "    with open(id_json, 'w') as json_file:\n",
    "        json.dump(ids, json_file)\n",
    "    \n",
    "    vectors = np.array(vectors)\n",
    "    vectors = normalize_rows(vectors)\n",
    "    \n",
    "    with open(vectors_pkl, 'bw') as pkl_file:\n",
    "        pickle.dump(vectors, pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_vectors_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a bug with loading large pickled files on MacOS, so id you try to run this locally on a Mac, it probably won't work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../vectors/vectors.pkl', 'rb') as vec_pkl:\n",
    "#     vectors = pickle.load(vec_pkl)\n",
    "    \n",
    "# with open('../../vectors/id.json', 'r') as ids_json:\n",
    "#     ids = json.load(ids_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Scores\n",
    "\n",
    "The functions in this subsection serve the following purpose\n",
    "\n",
    "1. `score` - compute a batch similarity scores \n",
    "1. `sort_scores` - sort the similarity scores, keeping in mind which articles the scores are associated with\n",
    "1. `compute_recs_paper_id` - A convenience function for computing the recommendations of an article by specifying the id of the article. This will be used in the Flask app for online computation of recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(vectors, all_ids, id_low, id_high):\n",
    "    \"\"\"\n",
    "    Outputs an array, scores. The columns of scores \n",
    "    are correspond to the slice of article_ids \n",
    "    \n",
    "    all_id[id_low:id_high]\n",
    "    \n",
    "    The rows are indexed by all_ids.\n",
    "    \n",
    "    The value in column C and row R is the \n",
    "    similarity between the articles all_ids[R]\n",
    "    and all_ids[id_low+C].\n",
    "    \"\"\"\n",
    "    mask = [id_low <= index < id_high for index, _ in enumerate(all_ids)]\n",
    "    rows = vectors[mask]\n",
    "    scores = vectors @ rows.T\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_scores(scores, all_ids, cur_ids):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of lists. The keys are the ids\n",
    "    of articles we're currently evaluating.\n",
    "\n",
    "    The lists contain tuples of scores and ids. The\n",
    "    score is the similarity score between the current article\n",
    "    and the other component of the tuple.\n",
    "    \"\"\"\n",
    "    recs_index = scores.argsort(0)[-51:,:][::-1]\n",
    "    recs = {\n",
    "      id:[(all_ids[index], scores[index][0]) for index in recs_index[:, col_num]]\n",
    "        for col_num, id in enumerate(cur_ids)\n",
    "    }\n",
    "    return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_recs_paper_id(arxiv_id, ids, vectors):\n",
    "    \"\"\"\n",
    "    Takes the list of id_number in question, \n",
    "    the list of all ids, and array of vectors\n",
    "    and returns a dictionary whose key is the arxiv_id\n",
    "    and values a list of the recommended articles and scores  \n",
    "    \"\"\"\n",
    "    \n",
    "    id_num = ids.index(arxiv_id)\n",
    "    scored = score(vectors, ids, id_num, id_num+1)\n",
    "    recs = sort_scores(scored, ids, ids[id_num:id_num+1])\n",
    "    return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %time recs = compute_recs_paper_id('1801.08262', ids, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(recs['1801.08262'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0000000000000002, '1801.08262'),\n",
       " (0.96994439808605737, '1212.2697'),\n",
       " (0.9675059791477697, '1410.0230'),\n",
       " (0.96315506293288022, '1611.03840'),\n",
       " (0.96170088971694145, '1705.00164')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs['1801.08262'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with SQL\n",
    "\n",
    "We'll define some functions that allow us to store the recommendations in a database so we don't have to compute them again when they're requested by the flask app. In particular the flask app should be able to perform two specific operations with the database.\n",
    "\n",
    "1. We should be able to retrieve check if a record exists in the table, and if it does render something for the user.\n",
    "2. If there is not matching record in the table, we can update the table and then service the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arxiv_imports.sqlalchemy_arxiv import Session, articles_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_server(arxiv_id, recs, table_class, session):\n",
    "    \"\"\"\n",
    "    Sends computed recommendations to the the database\n",
    "    \"\"\"\n",
    "    new_recs = [{'id':key,'recs':value} for key, value in recs.items()]\n",
    "    new_recs = [table_class(**args) for args in new_recs]\n",
    "    session.add_all(new_recs)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the new recommendations to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "send_to_server('1801.08262', recs, articles_similar, session)\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the records back from the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_recs(id_request, table_class_recs, session):\n",
    "    \"\"\"\n",
    "    Retrieves computed recommendations to the the database\n",
    "    \"\"\"\n",
    "    query = (session\n",
    "             .query(table_class_recs)\n",
    "             .filter(table_class_recs.id==id_request)\n",
    "            )\n",
    "    records = query.all()\n",
    "    if records:\n",
    "        return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "recs_ = request_recs('1801.08262', articles_similar, session)\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0000000000000002, '1801.08262'],\n",
       " [0.9699443980860574, '1212.2697'],\n",
       " [0.9675059791477697, '1410.0230'],\n",
       " [0.9631550629328802, '1611.03840'],\n",
       " [0.9617008897169415, '1705.00164']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs_[0].recs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random articles results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
