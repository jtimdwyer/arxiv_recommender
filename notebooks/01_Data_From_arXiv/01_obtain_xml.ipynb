{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## arXiv OAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "#We will use the lxml library for BeautifulSoup so you'll need that to run this. \n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The arxiv OAI API has some timeouts that can happen\n",
    "#they want us to wait 10 seconds when this happens\n",
    "#so I'm just hardcoding this here.\n",
    "\n",
    "def timed_request(url, params, wait_time, max_tries):\n",
    "    retry_counter = 0\n",
    "    while retry_counter < max_tries:\n",
    "        req = requests.get(url=url, params=params)\n",
    "    \n",
    "        if req.status_code == 200:\n",
    "            req = BeautifulSoup(req.text, 'lxml-xml')\n",
    "            return req\n",
    "        else:\n",
    "            retry_counter += 1\n",
    "            time.sleep(wait_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the XML obtained from the API to the disk\n",
    "\n",
    "def save_request(req, number, directory):\n",
    "    req = str(req)\n",
    "    \n",
    "    with open(f'{directory}/{number}.xml', 'w') as file:\n",
    "        file.write(req)\n",
    "        \n",
    "    return number + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to pass slightly different parameters when \n",
    "#making the first request, namely we have to pick the format \n",
    "#to return the data, this is the metadata_prefix\n",
    "\n",
    "def first_request(base_url, verb, metadata_prefix, wait_time, max_tries):\n",
    "    params = {\n",
    "        'verb':verb,\n",
    "        'metadataPrefix':metadata_prefix,\n",
    "             }\n",
    "    req = timed_request(url=base_url, params=params, wait_time=wait_time, max_tries=max_tries)\n",
    "    return req\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a simple utility to write messages to a file\n",
    "#mainly used here to check for resumption tokens\n",
    "\n",
    "def log(message, log_file):\n",
    "    with open(log_file, 'a') as file:\n",
    "        file.write(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Request Function\n",
    "\n",
    "The function `copy_oai` will handle all of our requests automatically and save our XML files in `../../data/xml/initial_harvest_YYYY_MM_DD/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_oai(base_url, verb, metadata_prefix=None, wait_time=10, max_tries=5,\n",
    "             log_directory='../../data/oai_logs', save_directory='../../data/xml',\n",
    "             resumption_token=None, request_num = 0):\n",
    "    \n",
    "        \n",
    "    #setting up a log file\n",
    "    log_name = str(datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\"))\n",
    "    log_file = f\"{log_directory}/{log_name}.log\"\n",
    "    \n",
    "    \n",
    "    #the save directory probably doesn't exist yet!\n",
    "    #we should make it\n",
    "    Path(save_directory).mkdir(exist_ok=True)    \n",
    "\n",
    "    \n",
    "    \n",
    "    #make the initial request assuming we aren't just using\n",
    "    #a resumption token\n",
    "    \n",
    "    if not resumption_token:\n",
    "        \n",
    "        log_str = 'Making first request without resumption token\\n'\n",
    "        log(log_str, log_file)\n",
    "        \n",
    "        first_get = first_request(base_url=base_url, verb=verb, metadata_prefix=metadata_prefix, \n",
    "                                 wait_time=wait_time, max_tries=max_tries)\n",
    "\n",
    "\n",
    "        if first_get:\n",
    "            log_str = [f'First request SUCCESSFUL, using resumption tokens going forward.\\n',\n",
    "                       f'Saving current object at {request_num}.xml\\n']\n",
    "            log_str = ''.join(log_str)\n",
    "            log(log_str, log_file)            \n",
    "\n",
    "            request_num = save_request(first_get, request_num, save_directory)\n",
    "            \n",
    "            resumption_token = first_get.find('resumptionToken')\n",
    "        \n",
    "            if not resumption_token:\n",
    "                log_str = 'No resumption token from first request, exiting.\\n'\n",
    "                log(log_str, log_file)            \n",
    "                return\n",
    "            \n",
    "        else:\n",
    "            log_str = 'First request failed, bailing out.\\n'\n",
    "            log(log_str, log_file)            \n",
    "            return\n",
    "    \n",
    "    \n",
    "    my_params = {\n",
    "        'verb': verb,\n",
    "        'resumptionToken': resumption_token\n",
    "    }\n",
    "    \n",
    "    \n",
    "    #start using resumption tokens we'll break out of this\n",
    "    #loop once we get a response that has no resumption token\n",
    "    \n",
    "    while my_params['resumptionToken']:\n",
    "        \n",
    "        if type(my_params['resumptionToken']) is not str:\n",
    "            my_params['resumptionToken'] = my_params['resumptionToken'].text\n",
    "            \n",
    "        log_time = str(datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\"))\n",
    "        log_str = f'Time: {log_time}, Resumption Token: {my_params[\"resumptionToken\"]}\\n'\n",
    "        log(log_str, log_file)            \n",
    "\n",
    "        time.sleep(wait_time)\n",
    "        next_request =  timed_request(url=base_url, params=my_params,\n",
    "                                  wait_time=wait_time, max_tries=max_tries)\n",
    "        \n",
    "\n",
    "        if next_request:\n",
    "            log_str = [f'Request SUCCESSFUL using Resumption Token {my_params[\"resumptionToken\"]}\\n',\n",
    "                       f'Saving current object at {request_num}.xml\\n']\n",
    "            log_str = ''.join(log_str)\n",
    "            \n",
    "            log(log_str, log_file)\n",
    "            \n",
    "            request_num = save_request(next_request, request_num, save_directory)\n",
    "        else: \n",
    "            log_str = f'Request FAILED using Resumption Token {my_params[\"resumptionToken\"]}\\n'\n",
    "            log(log_str, log_file)            \n",
    "\n",
    "        my_params['resumptionToken'] = next_request.find('resumptionToken')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I set this up so that if this process broke, I could look at the log function to find the `resumption_token` and `request_num` to resume the download without messing with the already downloaded files. None of this is hard coded so if you run this notebook, and none of the dependencies have changed in any meaningful way, you should end up with a complete copy of the arXiv metadata in the form of XML files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://export.arxiv.org/oai2'\n",
    "verb = 'ListRecords'\n",
    "metadata_prefix = 'arXiv'\n",
    "\n",
    "today = str(datetime.date.today()).replace('-','_')\n",
    "save_directory = f'../../data/xml/initial_harvest_{today}'\n",
    "\n",
    "copy_oai(base_url=base_url, verb=verb, metadata_prefix=metadata_prefix, save_directory=save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, if you find that the above process halted, you could look to the file `../../data/oai_log/YYYY_MM_DD_HH_mm_SS` (year, month, day hour, minute, second with a specified number of digits) to find the `resumption_token` that failed and `request_num` that it was. These can then be passed back to `copy_oai` to resume from that point rather than from the beginning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request_num = \n",
    "# resumption_token = \n",
    "\n",
    "# base_url = 'https://export.arxiv.org/oai2'\n",
    "# verb = 'ListRecords'\n",
    "\n",
    "\n",
    "\n",
    "# today = str(datetime.date.today()).replace('-','_')\n",
    "# save_directory = f'../../data/xml/initial_harvest_{today}'\n",
    "\n",
    "\n",
    "\n",
    "# copy_oai(base_url=base_url, verb=verb, save_directory=save_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
