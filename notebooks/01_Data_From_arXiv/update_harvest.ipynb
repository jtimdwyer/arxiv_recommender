{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Harvests\n",
    "\n",
    "In this notebook I show the process for updating our collection of __arXiv__ data using the OAI API. \n",
    "\n",
    "In particular we have two main things going on here:\n",
    "1. Acquire new XML files to update our data, we can accomplish this by looking to the most recent update to our data and passing that date along in our API call and get every article since then.\n",
    "1. Process our XML into JSON files, keeping track of exceptional records.\n",
    "\n",
    "To this end we will reuse most of the code from the other two notebooks in this directory but wrap it all in one function that should figure out the date of the most recent call on it's own. We do this by accessing the file `harvest_info.txt` that I manually created with the date of the initial harvest and will update with the most recent harvest date when we run the code in this notebook. \n",
    "\n",
    "So right now the file reads \n",
    "\n",
    ">first harvest  \n",
    ">18-06-2018\n",
    "\n",
    "and after running an update, say on June 29th 2018 will change it to read\n",
    "\n",
    ">first harvest  \n",
    ">2018-06-18  \n",
    ">update harvest  \n",
    ">2018-06-29 \n",
    "\n",
    "__You should be able to just run every cell in this notebook to update the XML and JSON files. HOWEVER, the required files are not actually present in the repo because they take up too much space. If you'd like to check this out yourself, feel free to email me.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from lxml import etree\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "#using the lxml library so you'll need that to run this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timed_request(url, params, wait_time, max_tries):\n",
    "    retry_counter = 0\n",
    "    while retry_counter < max_tries:\n",
    "        req = requests.get(url=url, params=params)\n",
    "    \n",
    "        if req.status_code == 200:\n",
    "            req = BeautifulSoup(req.text, 'lxml-xml')\n",
    "            return req\n",
    "        \n",
    "        else:\n",
    "            retry_counter += 1\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "\n",
    "def save_request(req, number, directory,):\n",
    "    req = str(req)\n",
    "    \n",
    "    with open(f'{directory}/{number}.xml', 'w') as file:\n",
    "        file.write(req)\n",
    "        \n",
    "    return number + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_request(base_url, verb, metadata_prefix, wait_time, max_tries, from_date='2018-06-19'):\n",
    "    params = {\n",
    "        'verb':verb,\n",
    "        'metadataPrefix':metadata_prefix,\n",
    "        'from':from_date,\n",
    "             }\n",
    "    req = timed_request(url=base_url, params=params, wait_time=wait_time, max_tries=max_tries)\n",
    "    return req\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(message, log_file):\n",
    "    with open(log_file, 'a') as file:\n",
    "        file.write(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_oai(base_url, verb, metadata_prefix=None, wait_time=10, max_tries=5,\n",
    "             log_directory='../../data/oai_logs', save_directory='../../data/xml',\n",
    "             resumption_token=None, request_num = 0, from_date = '2018-06-19', today=None):\n",
    "    \n",
    "        \n",
    "    log_name = str(datetime.datetime.now())\n",
    "    log_name = re.sub('[^0-9]', '_', log_name)  \n",
    "    log_file = f\"{log_directory}/{log_name}.log\"\n",
    "\n",
    "    if not resumption_token:\n",
    "        \n",
    "        log_str = 'Making first request without resumption token\\n'\n",
    "        log(log_str, log_file)\n",
    "        \n",
    "        first_get = first_request(base_url=base_url, verb=verb, metadata_prefix=metadata_prefix, \n",
    "                                 wait_time=wait_time, max_tries=max_tries, from_date=from_date)\n",
    "\n",
    "\n",
    "        if first_get:\n",
    "            log_str = [f'First request SUCCESSFUL, using resumption tokens going forward.\\n',\n",
    "                       f'Saving current object at {request_num}.xml\\n']\n",
    "            log_str = ''.join(log_str)\n",
    "            log(log_str, log_file)            \n",
    "\n",
    "            request_num = save_request(first_get, request_num, save_directory)\n",
    "            \n",
    "            resumption_token = first_get.find('resumptionToken')\n",
    "        \n",
    "            if not resumption_token:\n",
    "                log_str = 'No resumption token from first request, exiting.\\n'\n",
    "                log(log_str, log_file)            \n",
    "                return\n",
    "            \n",
    "        else:\n",
    "            log_str = 'First request failed, bailing out.\\n'\n",
    "            log(log_str, log_file)            \n",
    "            return\n",
    "    \n",
    "    \n",
    "    my_params = {\n",
    "        'verb': verb,\n",
    "        'resumptionToken': resumption_token\n",
    "    }\n",
    "    \n",
    "    while my_params['resumptionToken']:\n",
    "        \n",
    "        if type(my_params['resumptionToken']) is not str:\n",
    "            my_params['resumptionToken'] = my_params['resumptionToken'].text\n",
    "        \n",
    "        log_str = f'Time: {str(datetime.datetime.now())}, Resumption Token: {my_params[\"resumptionToken\"]}\\n'\n",
    "        log(log_str, log_file)            \n",
    "\n",
    "        time.sleep(wait_time)\n",
    "        next_request =  timed_request(url=base_url, params=my_params,\n",
    "                                  wait_time=wait_time, max_tries=max_tries)\n",
    "        \n",
    "\n",
    "        if next_request:\n",
    "            log_str = [f'Request SUCCESSFUL using Resumption Token {my_params[\"resumptionToken\"]}\\n',\n",
    "                       f'Saving current object at {request_num}.xml\\n']\n",
    "            log_str = ''.join(log_str)\n",
    "            \n",
    "            log(log_str, log_file)\n",
    "            \n",
    "            request_num = save_request(next_request, request_num, save_directory)\n",
    "        else: \n",
    "            log_str = f'Request FAILED using Resumption Token {my_params[\"resumptionToken\"]}\\n'\n",
    "            log(log_str, log_file)            \n",
    "\n",
    "        my_params['resumptionToken'] = next_request.find('resumptionToken')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harvest_update():\n",
    "    #first to update our information we'll find the date we need to send as a\n",
    "    #from paramaeter and to see where we should save everything\n",
    "    \n",
    "    today = str(datetime.date.today())\n",
    "    \n",
    "    #create a directory to store the saved xmls\n",
    "    save_directory = f'../../data/xml/update_harvests/{today}'\n",
    "    \n",
    "    #being overly cautious here maybe, checking that the directory isn't there before we make it\n",
    "    #this is to avoid things being annoying if you have to run the process twice and Path.mkdir() fails\n",
    "    # when the directory is present already\n",
    "    if not os.path.isdir(save_directory):\n",
    "        Path(save_directory).mkdir()\n",
    "    \n",
    "    #find the most recent harvest date\n",
    "    with open('../../data/harvest_info.txt') as file:\n",
    "        last_harvest_date = file.readlines()[-1].strip()\n",
    "    \n",
    "    base_url = 'https://export.arxiv.org/oai2'\n",
    "    verb = 'ListRecords'\n",
    "    metadata_prefix = 'arXiv'\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    copy_oai(base_url=base_url, verb=verb, metadata_prefix=metadata_prefix,\n",
    "             save_directory=save_directory, today=today, from_date=last_harvest_date)\n",
    "\n",
    "\n",
    "\n",
    "    #update the harvest record\n",
    "    with open('../../data/harvest_info.txt', 'a') as file:\n",
    "        file.write(f'\\nupdate harvest\\n{today}\\n')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvest_update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These XML files have been saved to `../../data/xml/update_harvests/YYYY-MM-DD/FILE_NUM.xml` where `FILE_NUM` is just the ordering of the requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we process the XML files into JSON files.\n",
    "\n",
    "Just like the XMLs we will keep track of the dates of the updates within the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def header_parse(header):\n",
    "    new_header = {}\n",
    "    for child in header.getchildren():\n",
    "        if child.tag in new_header:\n",
    "            new_header[child.tag].append(child.text)\n",
    "        else:\n",
    "            new_header[child.tag] = [child.text]\n",
    "\n",
    "    return new_header\n",
    "\n",
    "def metadata_parse(metadata):\n",
    "    new_meta = {}\n",
    "    for child in metadata.getchildren()[0].getchildren():\n",
    "        if (child.tag in new_meta) and (child.tag[-7:] != 'authors'):\n",
    "            new_meta[child.tag].append(child.text)\n",
    "        elif (child.tag not in new_meta) and (child.tag[-7:] != 'authors'):\n",
    "            new_meta[child.tag] = [child.text]\n",
    "        else:\n",
    "            \n",
    "            #in this case child has each authors data as children, so we'll pass it to the authors_parse\n",
    "            #function to handle. This can be a little annoyting because not everything is standardized\n",
    "            \n",
    "            authors = authors_parse(child)\n",
    "            new_meta[child.tag] = authors\n",
    "\n",
    "    return new_meta\n",
    "\n",
    "\n",
    "def authors_parse(authors):\n",
    "    authors = authors.getchildren()\n",
    "    new_author_data = []\n",
    "    \n",
    "    for author in authors:\n",
    "        one_author = {}\n",
    "        for datum in author.getchildren():\n",
    "            one_author[datum.tag] = datum.text\n",
    "    \n",
    "        new_author_data.append(one_author)\n",
    "        \n",
    "    return new_author_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_arXiv_xml_json(xml_tree, exceptions=[]):\n",
    "\n",
    "    responseDate, request, ListRecords = xml_tree.getchildren()\n",
    "    arxiv_json = {\n",
    "        'responseDate': responseDate.text,\n",
    "        'request': request.text,\n",
    "        'ListRecords':[]\n",
    "    }\n",
    "\n",
    "\n",
    "    #the final obejct in ListRecords is not a real record but a resumption token which we don't really care about\n",
    "    for record in ListRecords[:-1]:\n",
    "        \n",
    "#       At least one of these records gets passed without any metadata :(, in the event that this happens\n",
    "#       we will just skip that entry\n",
    "\n",
    "        try:\n",
    "            header, metadata = record.getchildren()\n",
    "        except:\n",
    "            exceptions.append(record)\n",
    "\n",
    "        #this splits the header into pieces, which as far as I can tell have no children\n",
    "        #these are just the identifier, the datestamp and setspec\n",
    "        #since the setspec at least can have multiple values I'm going to maybe be overlly cautious here about \n",
    "        #keeping track of all of this header data\n",
    "        try:\n",
    "            header = header_parse(header)\n",
    "            metadata = metadata_parse(metadata)\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        record_json = {\n",
    "            'header':header,\n",
    "            'metadata':metadata,\n",
    "        }\n",
    "\n",
    "        arxiv_json['ListRecords'].append(record_json)\n",
    "    \n",
    "    return arxiv_json, exceptions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xmls(xml_directory='../../data/xml/initial_harvest_18_06_2018',\n",
    "                 json_directory='../../data/json/initial_harvest_18_06_2018',):\n",
    "    \n",
    "    #creates a list of strings of everything in xml_directory\n",
    "    xmls = os.listdir(xml_directory)\n",
    "    \n",
    "    #in case there's some hidden files running around in xml_directory, \n",
    "    #the original download was structured in such a way that we can easily pattern match the \n",
    "    #file names to make sure we're only looking at the files we want\n",
    "    # ALl of these files are just a string of \n",
    "    \n",
    "    #specifically there is a .ipynb_checkpoint directory in here that gets CREATED while running this script in \n",
    "    #a jupyter notebook. I guess that shouldn't come up as a huge problem, but for reusability, and my own sanity\n",
    "    #I'm going to just account for it here. \n",
    "    xmls = [file_name for file_name in xmls if re.match('\\d*.xml$', file_name)]\n",
    "\n",
    "    \n",
    "    exceptions = []\n",
    "    \n",
    "    for xml_file_name in xmls:\n",
    "        xml_path = f'{xml_directory}/{xml_file_name}'\n",
    "        \n",
    "        with open(xml_path, 'r') as xml_file:\n",
    "            xml_tree = etree.parse(xml_file).getroot()\n",
    "        \n",
    "        json_version, exceptions = convert_arXiv_xml_json(xml_tree, exceptions)\n",
    "        \n",
    "        json_file_name = xml_file_name.split('.')[0]\n",
    "        json_file_name = f'{json_file_name}.json'\n",
    "        \n",
    "        json_path = f'{json_directory}/{json_file_name}'\n",
    "        \n",
    "        with open(json_path, 'w') as json_file:\n",
    "            json.dump(json_version, json_file)\n",
    "    return exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_update():\n",
    "    today = str(datetime.date.today())\n",
    "    \n",
    "    direcs = {\n",
    "        'xml_directory': f'../../data/xml/update_harvests/{today}',\n",
    "        'json_directory': f'../../data/json/update_harvests/{today}',\n",
    "    }\n",
    "    \n",
    "    for direc in direcs.values():\n",
    "        if not os.path.isdir(direc):\n",
    "            Path(direc).mkdir()\n",
    "\n",
    "    \n",
    "    \n",
    "    exceptions = process_xmls(xml_directory=direcs['xml_directory'], json_directory=direcs['json_directory'])\n",
    "    \n",
    "    if exceptions:\n",
    "        with open(f'../../data/json/exceptional_records/{today}.json', 'w') as file:\n",
    "            json.dump(exceptions, file) \n",
    "        print('Found some exceptions, wild!')\n",
    "    else:\n",
    "        print('No exceptions! Seems like the update was fine.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No exceptions! Seems like the update was fine.\n"
     ]
    }
   ],
   "source": [
    "process_update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
