{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import dill\n",
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build a json with the following structure\n",
    "```\n",
    "{\n",
    "    'responseDate':'2018-06-19T02:56:50Z',\n",
    "    'request': 'http://export.arxiv.org/oai2'\n",
    "    'ListRecords':\n",
    "        [\n",
    "          record_0,\n",
    "          record_1,\n",
    "          ...\n",
    "          record_999\n",
    "        ],\n",
    "}\n",
    "````\n",
    "\n",
    "Then each of the `record_i` is itself a json with all the infomation for that record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def header_parse(header):\n",
    "    new_header = {}\n",
    "    for child in header.getchildren():\n",
    "        if child.tag in new_header:\n",
    "            new_header[child.tag].append(child.text)\n",
    "        else:\n",
    "            new_header[child.tag] = [child.text]\n",
    "\n",
    "    return new_header\n",
    "\n",
    "def metadata_parse(metadata):\n",
    "    new_meta = {}\n",
    "    for child in metadata.getchildren()[0].getchildren():\n",
    "        if (child.tag in new_meta) and (child.tag[-7:] != 'authors'):\n",
    "            new_meta[child.tag].append(child.text)\n",
    "        elif (child.tag not in new_meta) and (child.tag[-7:] != 'authors'):\n",
    "            new_meta[child.tag] = [child.text]\n",
    "        else:\n",
    "            \n",
    "            #in this case child has each authors data as children, so we'll pass it to the authors_parse\n",
    "            #function to handle. This can be a little annoyting because not everything is standardized\n",
    "            \n",
    "            authors = authors_parse(child)\n",
    "            new_meta[child.tag] = authors\n",
    "\n",
    "    return new_meta\n",
    "\n",
    "\n",
    "def authors_parse(authors):\n",
    "    authors = authors.getchildren()\n",
    "    new_author_data = []\n",
    "    \n",
    "    for author in authors:\n",
    "        one_author = {}\n",
    "        for datum in author.getchildren():\n",
    "            one_author[datum.tag] = datum.text\n",
    "    \n",
    "        new_author_data.append(one_author)\n",
    "        \n",
    "    return new_author_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_arXiv_xml_json(xml_tree, exceptions=[]):\n",
    "\n",
    "    responseDate, request, ListRecords = xml_tree.getchildren()\n",
    "    arxiv_json = {\n",
    "        'responseDate': responseDate.text,\n",
    "        'request': request.text,\n",
    "        'ListRecords':[]\n",
    "    }\n",
    "\n",
    "\n",
    "    #the final obejct in ListRecords is not a real record but a resumption token which we don't really care about\n",
    "    for record in ListRecords[:-1]:\n",
    "        \n",
    "#       At least one of these records gets passed without any metadata :(, in the event that this happens\n",
    "#       we will just skip that entry\n",
    "\n",
    "        try:\n",
    "            header, metadata = record.getchildren()\n",
    "        except:\n",
    "            exceptions.append(record)\n",
    "\n",
    "        #this splits the header into pieces, which as far as I can tell have no children\n",
    "        #these are just the identifier, the datestamp and setspec\n",
    "        #since the setspec at least can have multiple values I'm going to maybe be overlly cautious here about \n",
    "        #keeping track of all of this header data\n",
    "        try:\n",
    "            header = header_parse(header)\n",
    "            metadata = metadata_parse(metadata)\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        record_json = {\n",
    "            'header':header,\n",
    "            'metadata':metadata,\n",
    "        }\n",
    "\n",
    "        arxiv_json['ListRecords'].append(record_json)\n",
    "    \n",
    "    return arxiv_json, exceptions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open XML file, close after indented work\n",
    "> create a tree object\n",
    "1. create json from the tree\n",
    "1. save the json to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xmls(xml_directory='../../data/xml/initial_harvest_18_06_2018',\n",
    "                 json_directory='../../data/json/initial_harvest_18_06_2018',\n",
    "                 log_directory='../../data/'\n",
    "                ):\n",
    "    \n",
    "    #creates a list of strings of everything in xml_directory\n",
    "    xmls = os.listdir(xml_directory)\n",
    "    \n",
    "    #in case there's some hidden files running around in xml_directory, \n",
    "    #the original download was structured in such a way that we can easily pattern match the \n",
    "    #file names to make sure we're only looking at the files we want\n",
    "    # ALl of these files are just a string of \n",
    "    \n",
    "    #specifically there is a .ipynb_checkpoint directory in here that gets CREATED while running this script in \n",
    "    #a jupyter notebook. I guess that shouldn't come up as a huge problem, but for reusability, and my own sanity\n",
    "    #I'm going to just account for it here. \n",
    "    xmls = [file_name for file_name in xmls if re.match('\\d*.xml$', file_name)]\n",
    "\n",
    "    \n",
    "    exceptions = []\n",
    "    \n",
    "    for xml_file_name in xmls:\n",
    "        xml_path = f'{xml_directory}/{xml_file_name}'\n",
    "        \n",
    "        with open(xml_path, 'r') as xml_file:\n",
    "            xml_tree = etree.parse(xml_file).getroot()\n",
    "        \n",
    "        json_version, exceptions = convert_arXiv_xml_json(xml_tree, exceptions)\n",
    "        \n",
    "        json_file_name = xml_file_name.split('.')[0]\n",
    "        json_file_name = f'{json_file_name}.json'\n",
    "        \n",
    "        json_path = f'{json_directory}/{json_file_name}'\n",
    "        \n",
    "        with open(json_path, 'w') as json_file:\n",
    "            json.dump(json_version, json_file)\n",
    "    return exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions = process_xmls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exceptions list should contain the records that didn't have both head data and metadata, we'll take a closer look at the entries of the list. A cursory investigation leads me to believe these are actually not errors, but a weird sort of record management that accounts for complications in the arXiv database. For example look at the id number below, and then follow [this link](https://arxiv.org/abs/1105.2364) to look up that paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exceptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oai:arXiv.org:1105.2364'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exceptions[0].getchildren()[0].getchildren()[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the exceptions so I cna look at them more carefully later. We can't pickle an `lxml` object (I think because `lxml` objects are not really python objects, we I'll just take the base strings pickle the list of those. They can all be passed through `lxml` again to restore the tree itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "exception_strs = [etree.tostring(record) for record in exceptions]\n",
    "with open('exceptions.pkl', 'bw') as exceptions_file:\n",
    "    dill.dump(exception_strs, exceptions_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
