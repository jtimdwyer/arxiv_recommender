{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build a json with the following structure\n",
    "```\n",
    "{\n",
    "    'responseDate':'2018-06-19T02:56:50Z',\n",
    "    'request': 'http://export.arxiv.org/oai2'\n",
    "    'ListRecords':\n",
    "        [\n",
    "          record_0,\n",
    "          record_1,\n",
    "          ...\n",
    "          record_999\n",
    "        ],\n",
    "}\n",
    "```\n",
    "\n",
    "Then each of the `record_i` is itself a json with all the infomation for that record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def header_parse(header):\n",
    "    new_header = {}\n",
    "    for child in header.getchildren():\n",
    "        if child.tag in new_header:\n",
    "            new_header[child.tag].append(child.text)\n",
    "        else:\n",
    "            new_header[child.tag] = [child.text]\n",
    "\n",
    "    return new_header\n",
    "\n",
    "def metadata_parse(metadata):\n",
    "    new_meta = {}\n",
    "    for child in metadata.getchildren()[0].getchildren():\n",
    "        if (child.tag in new_meta) and (child.tag[-7:] != 'authors'):\n",
    "            new_meta[child.tag].append(child.text)\n",
    "        elif (child.tag not in new_meta) and (child.tag[-7:] != 'authors'):\n",
    "            new_meta[child.tag] = [child.text]\n",
    "        else:\n",
    "            \n",
    "            #in this case child has each authors data as children, so we'll pass it to the authors_parse\n",
    "            #function to handle. This can be a little annoyting because not everything is standardized\n",
    "            \n",
    "            authors = authors_parse(child)\n",
    "            new_meta[child.tag] = authors\n",
    "\n",
    "    return new_meta\n",
    "\n",
    "\n",
    "def authors_parse(authors):\n",
    "    authors = authors.getchildren()\n",
    "    new_author_data = []\n",
    "    \n",
    "    for author in authors:\n",
    "        one_author = {}\n",
    "        for datum in author.getchildren():\n",
    "            one_author[datum.tag] = datum.text\n",
    "    \n",
    "        new_author_data.append(one_author)\n",
    "        \n",
    "    return new_author_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_arXiv_xml_json(xml_tree, exceptions=[]):\n",
    "\n",
    "    responseDate, request, ListRecords = xml_tree.getchildren()\n",
    "    arxiv_json = {\n",
    "        'responseDate': responseDate.text,\n",
    "        'request': request.text,\n",
    "        'ListRecords':[]\n",
    "    }\n",
    "\n",
    "\n",
    "    #the final obejct in ListRecords is not a real record but a resumption token which we don't really care about\n",
    "    for record in ListRecords[:-1]:\n",
    "        \n",
    "#       At least one of these records gets passed without any metadata :(, in the event that this happens\n",
    "#       we will just skip that entry\n",
    "\n",
    "        try:\n",
    "            header, metadata = record.getchildren()\n",
    "        except:\n",
    "            exceptions.append(record)\n",
    "\n",
    "        #this splits the header into pieces, which as far as I can tell have no children\n",
    "        #these are just the identifier, the datestamp and setspec\n",
    "        #since the setspec at least can have multiple values I'm going to maybe be overlly cautious here about \n",
    "        #keeping track of all of this header data\n",
    "        try:\n",
    "            header = header_parse(header)\n",
    "            metadata = metadata_parse(metadata)\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        record_json = {\n",
    "            'header':header,\n",
    "            'metadata':metadata,\n",
    "        }\n",
    "\n",
    "        arxiv_json['ListRecords'].append(record_json)\n",
    "    \n",
    "    return arxiv_json, exceptions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open XML file, close after indented work\n",
    "> create a tree object\n",
    "1. create json from the tree\n",
    "1. save the json to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xmls(xml_directory='../../data/xml/initial_harvest_18_06_2018',\n",
    "                 json_directory='../../data/json/initial_harvest_18_06_2018',\n",
    "                 log_directory='../../data/'\n",
    "                ):\n",
    "    \n",
    "    #creates a list of strings of everything in xml_directory\n",
    "    xmls = os.listdir(xml_directory)\n",
    "    \n",
    "    #in case there's some hidden files running around in xml_directory, \n",
    "    #the original download was structured in such a way that we can easily pattern match the \n",
    "    #file names to make sure we're only looking at the files we want\n",
    "    # ALl of these files are just a string of \n",
    "    \n",
    "    #specifically there is a .ipynb_checkpoint directory in here that gets CREATED while running this script in \n",
    "    #a jupyter notebook. I guess that shouldn't come up as a huge problem, but for reusability, and my own sanity\n",
    "    #I'm going to just account for it here. \n",
    "    xmls = [file_name for file_name in xmls if re.match('\\d*.xml$', file_name)]\n",
    "\n",
    "    \n",
    "    exceptions = []\n",
    "    \n",
    "    for xml_file_name in xmls:\n",
    "        xml_path = f'{xml_directory}/{xml_file_name}'\n",
    "        \n",
    "        with open(xml_path, 'r') as xml_file:\n",
    "            xml_tree = etree.parse(xml_file).getroot()\n",
    "        \n",
    "        json_version, exceptions = convert_arXiv_xml_json(xml_tree, exceptions)\n",
    "        \n",
    "        json_file_name = xml_file_name.split('.')[0]\n",
    "        json_file_name = f'{json_file_name}.json'\n",
    "        \n",
    "        json_path = f'{json_directory}/{json_file_name}'\n",
    "        \n",
    "        with open(json_path, 'w') as json_file:\n",
    "            json.dump(json_version, json_file)\n",
    "    return exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions = process_xmls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exceptions list should contain the records that didn't have both head data and metadata, there were only 11 such records and they all are deleted papers (because it was a duplicate of another in the arXiv). For example look at the `id` number below, and then follow [this link](https://arxiv.org/abs/1105.2364) to look up that paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exceptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oai:arXiv.org:1105.2364'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exceptions[0].getchildren()[0].getchildren()[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll keep all of the record json files that don't match the style we expect in `../../data/json/exceptional_records` as a file `date_of_acquisition.json`. When I set up a harvester program to do this for the incremental updates I'll store the future records of this type (I don't think there ever will be any) in there in the same way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "exception_strs = [etree.tostring(record) for record in exceptions]\n",
    "exception_strs = [ str(rec) for rec in exception_strs]\n",
    "exc_json = '../../data/json/exceptional_records/18_06_2018.json'\n",
    "\n",
    "with open(exc_json, 'w') as file:\n",
    "    json.dump(exception_strs_, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
