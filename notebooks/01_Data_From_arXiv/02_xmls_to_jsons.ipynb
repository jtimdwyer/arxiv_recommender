{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build a json with the following structure\n",
    "```\n",
    "{\n",
    "    'responseDate':'2018-06-19T02:56:50Z',\n",
    "    'request': 'http://export.arxiv.org/oai2'\n",
    "    'ListRecords':\n",
    "        [\n",
    "          record_0,\n",
    "          record_1,\n",
    "          ...\n",
    "          record_999\n",
    "        ],\n",
    "}\n",
    "```\n",
    "\n",
    "Then each of the `record_i` is itself a json with all the infomation for that record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the records have two constituent pieces, the header and metadata, these functions\n",
    "#header_parse and metadata_parse handle these XML tree objects and \n",
    "#transform into dictionaries of strings\n",
    "\n",
    "def header_parse(header):\n",
    "    new_header = {}\n",
    "    for child in header.getchildren():\n",
    "        if child.tag in new_header:\n",
    "            new_header[child.tag].append(child.text)\n",
    "        else:\n",
    "            new_header[child.tag] = [child.text]\n",
    "\n",
    "    return new_header\n",
    "\n",
    "def metadata_parse(metadata):\n",
    "    new_meta = {}\n",
    "    for child in metadata.getchildren()[0].getchildren():\n",
    "        if (child.tag in new_meta) and (child.tag[-7:] != 'authors'):\n",
    "            new_meta[child.tag].append(child.text)\n",
    "            \n",
    "        elif (child.tag not in new_meta) and (child.tag[-7:] != 'authors'):\n",
    "            new_meta[child.tag] = [child.text]\n",
    "        else:\n",
    "            \n",
    "            #in this case child has each authors data as children, so we'll pass it to the authors_parse\n",
    "            #function to handle. This can be a little annoyting because not everything is standardized\n",
    "            \n",
    "            authors = authors_parse(child)\n",
    "            new_meta[child.tag] = authors\n",
    "\n",
    "    return new_meta\n",
    "\n",
    "\n",
    "\n",
    "#the author data is a a a little involved since there can be many authors\n",
    "#the authors_parse function attempts to flatten out that information \n",
    "def authors_parse(authors):\n",
    "    authors = authors.getchildren()\n",
    "    new_author_data = []\n",
    "    \n",
    "    for author in authors:\n",
    "        one_author = {}\n",
    "        for datum in author.getchildren():\n",
    "            one_author[datum.tag] = datum.text\n",
    "    \n",
    "        new_author_data.append(one_author)\n",
    "        \n",
    "    return new_author_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_arXiv_xml_json(xml_tree, exceptions=[]):\n",
    "\n",
    "    responseDate, request, ListRecords = xml_tree.getchildren()\n",
    "    arxiv_json = {\n",
    "        'responseDate': responseDate.text,\n",
    "        'request': request.text,\n",
    "        'ListRecords':[]\n",
    "    }\n",
    "\n",
    "#         the final obejct in ListRecords is not a real record but a resumption token which we don't really care about\n",
    "#         which is why we're only iterating over ListRecords[:-1]\n",
    "    for record in ListRecords[:-1]:\n",
    "        \n",
    "#         At least one of these records gets passed without any metadata :(, in the event that this happens\n",
    "#         we will just skip that entry. These articles seem to only correspond to deleted entries from arXiv\n",
    "\n",
    "#         In the event that one of these functions that is meant to parse out the XMLs fails, we'll\n",
    "#         add the record to our list of exceptions.\n",
    "\n",
    "        try:\n",
    "            header, metadata = record.getchildren()\n",
    "            \n",
    "            header = header_parse(header)\n",
    "            metadata = metadata_parse(metadata)\n",
    "\n",
    "            record_json = {\n",
    "                'header':header,\n",
    "                'metadata':metadata,\n",
    "            }\n",
    "\n",
    "            arxiv_json['ListRecords'].append(record_json)\n",
    "\n",
    "        except:\n",
    "            exceptions.append(record)\n",
    "\n",
    "    return arxiv_json, exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open XML file, close after indented work\n",
    "> create a tree object\n",
    "1. create json from the tree\n",
    "1. save the json to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xmls(xml_directory, json_directory):\n",
    "    \n",
    "    #create the jsons directory, if it doesn't exist yet\n",
    "    Path(json_directory).mkdir(exist_ok=True)    \n",
    "    \n",
    "    #creates a list of strings of everything in xml_directory\n",
    "    xmls = os.listdir(xml_directory)\n",
    "    \n",
    "    #in case there's some hidden files running around in xml_directory, \n",
    "    #the original download was structured in such a way that we can easily pattern match the \n",
    "    #file names to make sure we're only looking at the files we want\n",
    "    # ALl of these files are just a string of \n",
    "    \n",
    "    #specifically there is a .ipynb_checkpoint directory in here that gets CREATED while running this script in \n",
    "    #a jupyter notebook. I guess that shouldn't come up as a huge problem, but for reusability, and my own sanity\n",
    "    #I'm going to just account for it here. \n",
    "    \n",
    "    xmls = [file_name for file_name in xmls if re.match('\\d*.xml$', file_name)]\n",
    "\n",
    "    \n",
    "    exceptions = []\n",
    "    \n",
    "    for xml_file_name in xmls:\n",
    "        xml_path = f'{xml_directory}/{xml_file_name}'\n",
    "        \n",
    "        with open(xml_path, 'r') as xml_file:\n",
    "            xml_tree = etree.parse(xml_file).getroot()\n",
    "        \n",
    "        json_version, exceptions = convert_arXiv_xml_json(xml_tree, exceptions)\n",
    "        \n",
    "        json_file_name = xml_file_name.split('.')[0]\n",
    "        json_file_name = f'{json_file_name}.json'\n",
    "        \n",
    "        json_path = f'{json_directory}/{json_file_name}'\n",
    "        \n",
    "        with open(json_path, 'w') as json_file:\n",
    "            json.dump(json_version, json_file)\n",
    "    return exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you try this yourself, you should change the date below to match the date \n",
    "#you ran your initial harvest\n",
    "\n",
    "harvest_date = '2018_06_21'\n",
    "\n",
    "xml_directory = f'../../data/xml/initial_harvest_{harvest_date}/'\n",
    "json_directory = f'../../data/json/initial_harvest_{harvest_date}'\n",
    "\n",
    "exceptions = process_xmls(xml_directory, json_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exceptions list should contain the records that didn't have both head data and metadata, there were only 11 such records and they all are deleted papers (because it was a duplicate of another in the arXiv). For example look at the `id` number below, and then follow [this link](https://arxiv.org/abs/1105.2364) to look up that paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exceptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oai:arXiv.org:1105.2364'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exceptions[0].getchildren()[0].getchildren()[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll keep all of the record json files that don't match the style we expect in `../../data/json/exceptional_records` as a file `date_of_acquisition.json`. When I set up a harvester program to do this for the incremental updates I'll store the future records of this type (I don't think there ever will be any) in there in the same way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "exception_strs = [etree.tostring(record) for record in exceptions]\n",
    "exception_strs = [str(rec) for rec in exception_strs]\n",
    "\n",
    "exc_json = f'../../data/json/exceptional_records/{harvest_date}.json'\n",
    "\n",
    "with open(exc_json, 'w') as file:\n",
    "    json.dump(exception_strs_, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(exc_json).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
