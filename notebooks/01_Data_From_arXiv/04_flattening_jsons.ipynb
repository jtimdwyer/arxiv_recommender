{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatten everything\n",
    "\n",
    "JSONs are, to my mind, certainly preferable to the XML we got in the first place, but they're still not quite as flat as we'd like. We are going to take our JSONs and process them into a PostgreSQL server on AWS. There will be six columns in the table, as depicted below.\n",
    "\n",
    "| id | created | setspec | title | abstract | tex |\n",
    "|----|---------|---------|-------|----------|-----|\n",
    "| 0704.0001| 2007-04-02| physics:hep-ph| Calculation of prompt diphoton production cross sections at Tevatron and LHC energies| A fully differential calculation in perturbative quantum chromodynamics is presented for the production of massive photon pairs at hadron colliders. All next-to-leading order perturbative contributions from quark-antiquark  gluon-(anti)quark  and gluon-gluon subprocesses are included  as well as all-orders resummation of initial-state gluon radiation valid at next-to-next-to-leading logarithmic accuracy. The region of phase space is specified in which the calculation is most reliable. Good agreement is demonstrated with data from the Fermilab Tevatron  and predictions are made for more detailed tests with CDF and DO data. Predictions are shown for distributions of diphoton pairs produced at the energy of the Large Hadron Collider (LHC). Distributions of the diphoton pairs from the decay of a Higgs boson are contrasted with those produced from QCD processes at the LHC  showing that enhanced sensitivity to the signal can be obtained with judicious selection of events.| good| \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we handle the $LaTeX$\n",
    "\n",
    "On arXiv it's very common for abstracts, or even titles, to have at least a little $LaTeX$. This isn't ideal for our purposes, we'll go ahead and try to remove all the formatted equations and the english words that change the text styling (doing our best to leave the actual words unaffected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pypandoc\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove latex math\n",
    "# convert to plain text, this means that different bold, emphasis schemes are represented\n",
    "# symbolically rather than as text commands this is preferable \n",
    "# since we can remove those symbols without fear of affecting the words themselves!\n",
    "\n",
    "\n",
    "math_regex = '<span class=\"math.*?\">.*?</span>'\n",
    "math_compiled = re.compile(math_regex, flags=re.DOTALL)\n",
    "\n",
    "#there's maybe a simpler way of doing this...but this is straightforward :) \n",
    "\n",
    "def latex_clean(latex_string, re_compiled):\n",
    "\n",
    "    \n",
    "    #first we convert to html\n",
    "    cleaned_string = pypandoc.convert_text(source=latex_string, to='html', format='latex')\n",
    "    \n",
    "    #remove the math mode stuff\n",
    "    cleaned_string = re_compiled.sub(repl=' ', string=cleaned_string)\n",
    "\n",
    "    #convert to plaintext\n",
    "    cleaned_string = pypandoc.convert_text(source=cleaned_string, to='plain', format='html')\n",
    "    \n",
    "    #get rid of new lines\n",
    "    cleaned_string = cleaned_string.replace('\\n', ' ')\n",
    "    \n",
    "    return cleaned_string\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_csv(full_article_json):\n",
    "    \n",
    "    #the category information in the header seems a little cleaner\n",
    "    #than the one in the metadata\n",
    "    header = full_article_json['header']\n",
    "    metadata = full_article_json['metadata']\n",
    "    \n",
    "    key_prefix = '{http://www.openarchives.org/OAI/2.0/}'\n",
    "    \n",
    "    csv = {}\n",
    "    #I'm channging the name of id to arxivid because I want\n",
    "    csv['id'] = metadata[f'{key_prefix}id'][0]\n",
    "    csv['created'] = metadata[f'{key_prefix}created'][0]\n",
    "    csv['setspec'] = header[f'{key_prefix}setSpec'][0]\n",
    "    csv['title'] = metadata[f'{key_prefix}title'][0]\n",
    "    csv['abstract'] = metadata[f'{key_prefix}abstract'][0]\n",
    "    \n",
    "\n",
    "    #some of the abstracts or titles contain broken tex\n",
    "    #if this happens then we can't safely remove the tex automatically.\n",
    "    #We'll keep these in the exceptions list\n",
    "\n",
    "    \n",
    "    math_regex = '<span class=\"math.*?\">.*?</span>'\n",
    "    math_compiled = re.compile(math_regex, flags=re.DOTALL)\n",
    "\n",
    "    try:\n",
    "        #need to fix the title\n",
    "        csv['title'] = latex_clean(csv['title'], math_compiled).replace(',', ' ')\n",
    "\n",
    "        #need to fix the abstract\n",
    "        csv['abstract'] = latex_clean(csv['abstract'], math_compiled).replace(',', ' ')\n",
    "\n",
    "        csv['tex'] = 'good'\n",
    "        \n",
    "    except:\n",
    "        csv['tex'] = 'bad'\n",
    "        \n",
    "    return csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_lines(json_file_name):\n",
    "    \n",
    "    with open(json_file_name) as json_file:\n",
    "        jtmp = json.load(json_file)['ListRecords']\n",
    "    \n",
    "    article_info = []\n",
    "    key_prefix = '{http://www.openarchives.org/OAI/2.0/}'\n",
    "    \n",
    "    for jtmp_sample in jtmp:\n",
    "        new_entry = json_to_csv(jtmp_sample)\n",
    "        article_info.append(new_entry)\n",
    "    return article_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "A decent percentage of the articles have some $\\LaTeX$ that doesn't play nicely with `pandoc`. In some cases this is due to truly broken $\\TeX$. This is the reasoning behind the `tex` feature. If `pandoc` fails, for whatever reason, to convert the $\\TeX$ to __HTML__ or from __HTML__ onto plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_name = '../../data/json/initial_harvest_2018_06_21/0.json'\n",
    "article_info = json_to_lines(json_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(article_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_articles = {\n",
    "    'good':[],\n",
    "    'bad':[]\n",
    "}\n",
    "for artic in article_info:\n",
    "    sort_articles[artic['tex']].append(artic)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "895"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sort_articles['good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0704.0001',\n",
       " 'created': '2007-04-02',\n",
       " 'setspec': 'physics:hep-ph',\n",
       " 'title': 'Calculation of prompt diphoton production cross sections at Tevatron and LHC energies ',\n",
       " 'abstract': 'A fully differential calculation in perturbative quantum chromodynamics is presented for the production of massive photon pairs at hadron colliders. All next-to-leading order perturbative contributions from quark-antiquark  gluon-(anti)quark  and gluon-gluon subprocesses are included  as well as all-orders resummation of initial-state gluon radiation valid at next-to-next-to-leading logarithmic accuracy. The region of phase space is specified in which the calculation is most reliable. Good agreement is demonstrated with data from the Fermilab Tevatron  and predictions are made for more detailed tests with CDF and DO data. Predictions are shown for distributions of diphoton pairs produced at the energy of the Large Hadron Collider (LHC). Distributions of the diphoton pairs from the decay of a Higgs boson are contrasted with those produced from QCD processes at the LHC  showing that enhanced sensitivity to the signal can be obtained with judicious selection of events. ',\n",
       " 'tex': 'good'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_articles['good'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Postgres\n",
    "\n",
    "Moving everything over to our Postgres database hosted on AWS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, String, Integer, DATE\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "from sqlalchemy.ext.declarative import declarative_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./postgres.json') as pg_info:\n",
    "    pg_json = json.load(pg_info)\n",
    "    pg_username = pg_json['username']\n",
    "    pg_password = pg_json['password']\n",
    "    pg_ip = pg_json['ip']\n",
    "\n",
    "engine = create_engine(f'postgres://{pg_username}:{pg_password}@{pg_ip}:5432')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "postgres://postgres:***@52.39.221.147:5432"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "# the article class is how sqlalchemy treats the objects of a row\n",
    "class Articles(Base):\n",
    "    __tablename__ = 'arxiv'\n",
    "    \n",
    "    id = Column(String, primary_key=True)\n",
    "    created = Column(DATE)\n",
    "    setspec = Column(String)\n",
    "    title = Column(String)\n",
    "    abstract = Column(String)\n",
    "    tex = Column(String)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to atually put the table in the database\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def json_to_sql(json_dir, engine):\n",
    "    json_dir = Path(json_dir)\n",
    "    \n",
    "    Session = sessionmaker(bind=engine)\n",
    "    \n",
    "    for json_file_name in json_dir.iterdir():\n",
    "        if json_file_name.suffix == '.json': #make sure we've got the file, just in case.\n",
    "            session = Session()\n",
    "            articles = json_to_lines(json_file_name) \n",
    "\n",
    "            articles = [Articles(**article_info) for article_info in articles]\n",
    "\n",
    "            session.add_all(articles)\n",
    "\n",
    "            session.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = \"../../data/json/initial_harvest_2018_06_21\"\n",
    "\n",
    "json_to_sql(json_dir, engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
